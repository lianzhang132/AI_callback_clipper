{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train_all, y_train_all),(x_test,y_test) = fashion_mnist.load_data()\n",
    "x_valid,x_train = x_train_all[:5000],x_train_all[5000:]\n",
    "y_valid,y_train = y_train_all[:5000],y_train_all[5000:]\n",
    "\n",
    "print(\"归一化前的最大值和最小值\",np.max(x_train),np.min(x_train))\n",
    "\n",
    "#  数据归一化处理\n",
    "# x = (x - u ) / std\n",
    "# u 是均值 std 是方差 如此便得到了 均值是0 方差是 1 的正态分布了\n",
    "\n",
    "#用到了sklearn的api\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "#初始化一个 scaler对象 \n",
    "scaler = StandardScaler()\n",
    "#x_train: [None,28,28] -> [None,784] -> [-1,28,28]\n",
    "x_train_scaled = scaler.fit_transform(x_train.astype(np.float32).reshape(-1,1)).reshape(-1,28,28)\n",
    "#验证集 不需要做fit fit：是在训练集上得到均值和方差 把 均值和方差记录下来 因为做归一化的时候 是需要用训练集的均值和方差去做的 \n",
    "x_valid_scaled = scaler.transform(x_valid.astype(np.float32).reshape(-1,1)).reshape(-1,28,28)\n",
    "#测试集\n",
    "x_test_scaled = scaler.transform(x_test.astype(np.float32).reshape(-1,1)).reshape(-1,28,28)\n",
    "\n",
    "\n",
    "print(\"归一化之后的最大值和最小值\",np.max(x_train_scaled),np.min(x_train_scaled))\n",
    "\n",
    "#神经网络定义方式 \n",
    "# 1 ， 四层神经网略\n",
    "\"\"\"\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[28,28]))\n",
    "model.add(tf.keras.layers.Dense(300,activation='sigmoid'))\n",
    "model.add(tf.keras.layers.Dense(100,activation='sigmoid'))\n",
    "model.add(tf.keras.layers.Dense(10,activation='softmax'))\n",
    "\n",
    "\"\"\"\n",
    "#另外一种网络模型搭建的写法\n",
    "# model = tf.keras.models.Sequential([\n",
    "# #     tf.keras.layers.Dense(300,activation='sigmoid'),\n",
    "# #     tf.keras.layers.Dense(100,activation='sigmoid'),\n",
    "# #     tf.keras.layers.Dense(10,activation='softmax')\n",
    "# # ])\n",
    "\n",
    "# 多层神经wnaglu网略\n",
    "\n",
    "#relu : y = max(0,x)\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Flatten(input_shape = [28,28]))\n",
    "for  _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,activation = \"relu\"))\n",
    "model.add(keras.layers.Dense(10,activation = \"softmax\"))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
