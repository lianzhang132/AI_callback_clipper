{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-3c3a74a8552c>:41: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-1-3c3a74a8552c>:50: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The passed save_path is not a valid checkpoint: ./cifar_50/cifar10_model.ckpt-50",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3c3a74a8552c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./cifar_50/cifar10_model.ckpt-50\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_exists_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m       raise ValueError(\"The passed save_path is not a valid checkpoint: \" +\n\u001b[0;32m-> 1282\u001b[0;31m                        checkpoint_prefix)\n\u001b[0m\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The passed save_path is not a valid checkpoint: ./cifar_50/cifar10_model.ckpt-50"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "def weight(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape,stddev=0.1),name='W')\n",
    "def bias(shape):\n",
    "    return tf.Variable(tf.constant(0.1,shape=shape),name='b')\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "with tf.name_scope('input_layer'):\n",
    "    x = tf.placeholder('float',shape=[None,32,32,3],name='x')\n",
    "with tf.name_scope('conv_1'):\n",
    "    W1 = weight([3,3,3,32])\n",
    "    b1 = bias([32])\n",
    "    conv_1 = conv2d(x,W1)+b1\n",
    "    conv_1 = tf.nn.relu(conv_1)\n",
    "    \n",
    "with tf.name_scope('pool_1'):\n",
    "    pool_1 = max_pool_2x2(conv_1)\n",
    "    \n",
    "with tf.name_scope('conv_2'):\n",
    "    W2 = weight([3,3,32,64])\n",
    "    b2 = bias([64])\n",
    "    conv_2=conv2d(pool_1,W2)+b2\n",
    "    conv_2=tf.nn.relu(conv_2)\n",
    "    \n",
    "with tf.name_scope('pool_2'):\n",
    "    pool_2 = max_pool_2x2(conv_2)\n",
    "    \n",
    "with tf.name_scope('fc'):\n",
    "    W3 = weight([4096,128])\n",
    "    b3 = bias([128])\n",
    "    flat = tf.reshape(pool_2,[-1,4096])\n",
    "    h= tf.nn.relu(tf.matmul(flat,W3)+b3)\n",
    "    h_dropout = tf.nn.dropout(h,keep_prob=0.8)\n",
    "with tf.name_scope('output_layter'):\n",
    "    W4 = weight([128,10])\n",
    "    b4 = bias([10])\n",
    "    pred = tf.nn.softmax(tf.matmul(h_dropout,W4)+b4,name='pred')\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    y = tf.placeholder('float',shape=[None,10],name='label')\n",
    "    loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=pred,labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss_function)\n",
    "    \n",
    "\n",
    "with tf.name_scope('evaluation'):\n",
    "    correct_prediction = tf.equal(tf.argmax(pred,1),tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,'float'))\n",
    "    \n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "saver.restore(sess, \"./cifar_50/cifar10_model.ckpt-50\") \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    with open(filename,'rb')as f:\n",
    "        data_dict = pickle.load(f,encoding='bytes')\n",
    "        images = data_dict[b'data']\n",
    "        labels = data_dict[b'labels']\n",
    "        \n",
    "        #把元数据结构调整为：BCWH\n",
    "        images = images.reshape(10000,3,32,32)\n",
    "        #tesorflow 处理图像数据的结构为：BWHC\n",
    "        #把通道数据c移动到最后一个维度\n",
    "        images = images.transpose(0,2,3,1)\n",
    "        labels = np.array(labels)\n",
    "        return images,labels\n",
    "test_image,test_label = load_CIFAR_batch('./cifar10_data/cifar-10-batches-py/test_batch')\n",
    "print('显示原图',test_label[6])\n",
    "plt.imshow(test_image[6])\n",
    "\n",
    "\n",
    "import os\n",
    "def load_CIFAR_data(data_dir):\n",
    "    images_train = []\n",
    "    labels_train = []\n",
    "\n",
    "\n",
    "    for i in range(5):\n",
    "        # 循环读取，每一个文件的数据和标签名\n",
    "        f = os.path.join(data_dir,'data_batch_%d'%(i+1))\n",
    "        print('loading',f)\n",
    "        image_batch,label_batch=load_CIFAR_batch(f)\n",
    "        images_train.append(image_batch)\n",
    "        labels_train.append(label_batch)\n",
    "        Xtrain=np.concatenate(images_train)\n",
    "        Ytrain=np.concatenate(labels_train)\n",
    "        del image_batch,label_batch\n",
    "    Xtest,Ytest = load_CIFAR_batch(os.path.join(data_dir,'test_batch'))\n",
    "    print(\"finished loadding CIFAR-10 data\")\n",
    "    return Xtrain,Ytrain,Xtest,Ytest\n",
    "data_dir = './cifar10_data/cifar-10-batches-py/'\n",
    "\n",
    "Xtrain,Ytrain,Xtest,Ytest = load_CIFAR_data(data_dir)\n",
    "\n",
    "\n",
    "label_dict = {0:'airplane', 1:'automobile', 2:'bird', 3:'cat', 4:'deer', 5:'dog', 6:'frog', 7:'horse', 8:'ship', 9:'truck'}\n",
    "#定义显示图像数据以及对应标签的函数\n",
    "def plot_images_labels_prediction(images,labels,prediction,idx,num=10):\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(12,6)\n",
    "    if num > 10:\n",
    "        num=10\n",
    "    for i in range(0,num):\n",
    "        ax=plt.subplot(2,5,1+i)\n",
    "        ax.imshow(images[idx],cmap='binary')\n",
    "        \n",
    "        title = str(i)+','+label_dict[labels[idx]]\n",
    "        if len(prediction)>0:\n",
    "            title+='=>'+label_dict[prediction[idx]]\n",
    "        ax.set_title(title,fontsize=10)\n",
    "        idx+=1\n",
    "    plt.show()\n",
    "    \n",
    "plot_images_labels_prediction(Xtest,Ytest,[],1,10)\n",
    "\n",
    "Xtest_normalize = Xtest.astype('float32')/255.0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    with open(filename,'rb')as f:\n",
    "        data_dict = pickle.load(f,encoding='bytes')\n",
    "        images = data_dict[b'data']\n",
    "        labels = data_dict[b'labels']\n",
    "        \n",
    "        #把元数据结构调整为：BCWH\n",
    "        images = images.reshape(10000,3,32,32)\n",
    "        #tesorflow 处理图像数据的结构为：BWHC\n",
    "        #把通道数据c移动到最后一个维度\n",
    "        images = images.transpose(0,2,3,1)\n",
    "        labels = np.array(labels)\n",
    "        return images,labels\n",
    "test_image,test_label = load_CIFAR_batch('./cifar10_data/cifar-10-batches-py/test_batch')\n",
    "print('显示原图',test_label[6])\n",
    "plt.imshow(test_image[6])\n",
    "\n",
    "\n",
    "import os\n",
    "def load_CIFAR_data(data_dir):\n",
    "    images_train = []\n",
    "    labels_train = []\n",
    "\n",
    "\n",
    "    for i in range(5):\n",
    "        # 循环读取，每一个文件的数据和标签名\n",
    "        f = os.path.join(data_dir,'data_batch_%d'%(i+1))\n",
    "        print('loading',f)\n",
    "        image_batch,label_batch=load_CIFAR_batch(f)\n",
    "        images_train.append(image_batch)\n",
    "        labels_train.append(label_batch)\n",
    "        Xtrain=np.concatenate(images_train)\n",
    "        Ytrain=np.concatenate(labels_train)\n",
    "        del image_batch,label_batch\n",
    "    Xtest,Ytest = load_CIFAR_batch(os.path.join(data_dir,'test_batch'))\n",
    "    print(\"finished loadding CIFAR-10 data\")\n",
    "    return Xtrain,Ytrain,Xtest,Ytest\n",
    "data_dir = './cifar10_data/cifar-10-batches-py/'\n",
    "\n",
    "Xtrain,Ytrain,Xtest,Ytest = load_CIFAR_data(data_dir)\n",
    "\n",
    "\n",
    "label_dict = {0:'airplane', 1:'automobile', 2:'bird', 3:'cat', 4:'deer', 5:'dog', 6:'frog', 7:'horse', 8:'ship', 9:'truck'}\n",
    "#定义显示图像数据以及对应标签的函数\n",
    "def plot_images_labels_prediction(images,labels,prediction,idx,num=10):\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(12,6)\n",
    "    if num > 10:\n",
    "        num=10\n",
    "    for i in range(0,num):\n",
    "        ax=plt.subplot(2,5,1+i)\n",
    "        ax.imshow(images[idx],cmap='binary')\n",
    "        \n",
    "        title = str(i)+','+label_dict[labels[idx]]\n",
    "        if len(prediction)>0:\n",
    "            title+='=>'+label_dict[prediction[idx]]\n",
    "        ax.set_title(title,fontsize=10)\n",
    "        idx+=1\n",
    "    plt.show()\n",
    "    \n",
    "plot_images_labels_prediction(Xtest,Ytest,[],1,10)\n",
    "\n",
    "Xtest_normalize = Xtest.astype('float32')/255.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "\n",
    "saver =tf.train.import_meta_graph(\"cifar_50/cifar10_model.ckpt-50.meta\")\n",
    "saver.restore(sess, \"./cifar_50/cifar10_model.ckpt-50\") \n",
    "\n",
    "pred = tf.get_default_graph().get_tensor_by_name(\"pred:0\")\n",
    "x = tf.get_default_graph().get_tensor_by_name(\"x:0\")\n",
    "\n",
    "re = sess.run(pred,feed_dict={x:Xtest_normalize[:50]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文档写了两种取session的方式  一种需要完全重写一遍graph定义代码  且必须和源代码一致  \n",
    "另一种则不需要 重写graph定义代码 \n",
    "关于报取不到ckpt文件的错误  详情可见 https://www.cnblogs.com/monologuesmw/p/12273727.html 这篇博客写的很详细\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sess,inputs):\n",
    "#     preds = sess.run('pred:0', feed_dict={'X:0': inputs}) \n",
    "#     test_pred = sess.run(pred,feed_dict={x:inputs[:50]})\n",
    "    #部署clipper时用这种写法\n",
    "    test_pred = sess.run('pred:0',feed_dict={'x:0':inputs[:50]})\n",
    "\n",
    "    prediction_result =sess.run(tf.argmax(test_pred,1))\n",
    "    # `X` is used, it must be defined in the model with that name explicitly!\n",
    "    return prediction_result\n",
    "# predict(sess,Xtest_normalize)\n",
    "print(Xtest_normalize.shape)\n",
    "print(Xtest_normalize[0].shape)\n",
    "print(Xtest_normalize[0])\n",
    "\n",
    "\n",
    "# predict(sess,Xtest_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from clipper_admin import ClipperConnection, DockerContainerManager\n",
    "from clipper_admin.deployers.tensorflow import deploy_tensorflow_model\n",
    "clipper_conn = ClipperConnection(DockerContainerManager())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipper_conn.start_clipper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clipper_conn.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipper_conn.register_application(\n",
    "    name=\"tf-app\", input_type=\"doubles\", default_output=\"-1.0\", slo_micros=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_tensorflow_model(\n",
    "    clipper_conn,\n",
    "    name=\"tf-mod\",\n",
    "    version=1,\n",
    "    input_type=\"doubles\",\n",
    "    func=predict,\n",
    "    tf_sess_or_saved_model_path=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipper_conn.link_model_to_app(\n",
    "    app_name=\"tf-app\",\n",
    "    model_name=\"tf-mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_address = clipper_conn.get_query_addr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, numpy as np\n",
    "headers = {\"Content-type\": \"application/json\"}\n",
    "requests.post(\"http://\"+query_address+\"/tf-app/predict\", headers=headers, data=json.dumps({\n",
    "    \"input\": Xtest_normalize[0]})).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipper_conn.stop_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
