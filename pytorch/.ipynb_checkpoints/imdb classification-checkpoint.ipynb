{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import * \n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.datasets import imdb \n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 10000  # imdb’s vocab_size 即词汇表大小\n",
    "MAX_LEN = 200      # max length\n",
    "BATCH_SIZE = 256\n",
    "EMB_SIZE = 128   # embedding size\n",
    "HID_SIZE = 128   # lstm hidden size\n",
    "DROPOUT = 0.2 \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 200) (25000, 200)\n"
     ]
    }
   ],
   "source": [
    "# 借助Keras加载imdb数据集\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=MAX_WORDS)\n",
    "x_train = pad_sequences(x_train, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "x_test = pad_sequences(x_test, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转化为TensorDataset\n",
    "train_data = TensorDataset(torch.LongTensor(x_train), torch.LongTensor(y_train))\n",
    "test_data = TensorDataset(torch.LongTensor(x_test), torch.LongTensor(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转化为 DataLoader\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义lstm模型用于文本分类\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, max_words, emb_size, hid_size, dropout):\n",
    "        super(Model, self).__init__()\n",
    "        self.max_words = max_words\n",
    "        self.emb_size = emb_size\n",
    "        self.hid_size = hid_size\n",
    "        self.dropout = dropout\n",
    "        self.Embedding = nn.Embedding(self.max_words, self.emb_size)\n",
    "        self.LSTM = nn.LSTM(self.emb_size, self.hid_size, num_layers=2,\n",
    "                            batch_first=True, bidirectional=True)   # 2层双向LSTM\n",
    "        self.dp = nn.Dropout(self.dropout)\n",
    "        self.fc1 = nn.Linear(self.hid_size*2, self.hid_size)\n",
    "        self.fc2 = nn.Linear(self.hid_size, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input : [bs, maxlen]\n",
    "        output: [bs, 2] \n",
    "        \"\"\"\n",
    "        x = self.Embedding(x)  # [bs, ml, emb_size]\n",
    "        x = self.dp(x)\n",
    "        x, _ = self.LSTM(x)  # [bs, ml, 2*hid_size]\n",
    "        x = self.dp(x)\n",
    "        x = F.relu(self.fc1(x))   # [bs, ml, hid_size]\n",
    "        x = F.avg_pool2d(x, (x.shape[1], 1)).squeeze()  # [bs, 1, hid_size] => [bs, hid_size]\n",
    "        out = self.fc2(x)    # [bs, 2]\n",
    "        return out  # [bs, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):   # 训练模型\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        y_ = model(x)\n",
    "        loss = criterion(y_, y)  # 得到loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(batch_idx + 1) % 10 == 0:    # 打印loss\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(x), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):    # 测试模型\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')  # 累加loss\n",
    "    test_loss = 0.0 \n",
    "    acc = 0 \n",
    "    for batch_idx, (x, y) in enumerate(test_loader):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            y_ = model(x)\n",
    "        test_loss += criterion(y_, y)\n",
    "        pred = y_.max(-1, keepdim=True)[1]   # .max() 2输出，分别为最大值和最大值的index\n",
    "        acc += pred.eq(y.view_as(pred)).sum().item()    # 记得加item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, acc, len(test_loader.dataset),\n",
    "        100. * acc / len(test_loader.dataset)))\n",
    "    return acc / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (Embedding): Embedding(10000, 128)\n",
      "  (LSTM): LSTM(128, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (dp): Dropout(p=0.2)\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "Train Epoch: 1 [2304/25000 (9%)]\tLoss: 0.693110\n",
      "Train Epoch: 1 [4864/25000 (19%)]\tLoss: 0.683363\n",
      "Train Epoch: 1 [7424/25000 (30%)]\tLoss: 0.645504\n",
      "Train Epoch: 1 [9984/25000 (40%)]\tLoss: 0.595742\n",
      "Train Epoch: 1 [12544/25000 (50%)]\tLoss: 0.594726\n",
      "Train Epoch: 1 [15104/25000 (60%)]\tLoss: 0.593936\n",
      "Train Epoch: 1 [17664/25000 (70%)]\tLoss: 0.594907\n",
      "Train Epoch: 1 [20224/25000 (81%)]\tLoss: 0.499872\n",
      "Train Epoch: 1 [22784/25000 (91%)]\tLoss: 0.488271\n",
      "\n",
      "Test set: Average loss: 0.4733, Accuracy: 19486/25000 (78%)\n",
      "acc is: 0.7794, best acc is 0.7794\n",
      "\n",
      "Train Epoch: 2 [2304/25000 (9%)]\tLoss: 0.479104\n",
      "Train Epoch: 2 [4864/25000 (19%)]\tLoss: 0.420178\n",
      "Train Epoch: 2 [7424/25000 (30%)]\tLoss: 0.426991\n",
      "Train Epoch: 2 [9984/25000 (40%)]\tLoss: 0.410526\n",
      "Train Epoch: 2 [12544/25000 (50%)]\tLoss: 0.409595\n",
      "Train Epoch: 2 [15104/25000 (60%)]\tLoss: 0.375392\n",
      "Train Epoch: 2 [17664/25000 (70%)]\tLoss: 0.411750\n",
      "Train Epoch: 2 [20224/25000 (81%)]\tLoss: 0.416187\n",
      "Train Epoch: 2 [22784/25000 (91%)]\tLoss: 0.438418\n",
      "\n",
      "Test set: Average loss: 0.3749, Accuracy: 20898/25000 (84%)\n",
      "acc is: 0.8359, best acc is 0.8359\n",
      "\n",
      "Train Epoch: 3 [2304/25000 (9%)]\tLoss: 0.369996\n",
      "Train Epoch: 3 [4864/25000 (19%)]\tLoss: 0.375537\n",
      "Train Epoch: 3 [7424/25000 (30%)]\tLoss: 0.317179\n",
      "Train Epoch: 3 [9984/25000 (40%)]\tLoss: 0.375449\n",
      "Train Epoch: 3 [12544/25000 (50%)]\tLoss: 0.380360\n",
      "Train Epoch: 3 [15104/25000 (60%)]\tLoss: 0.352269\n",
      "Train Epoch: 3 [17664/25000 (70%)]\tLoss: 0.381747\n",
      "Train Epoch: 3 [20224/25000 (81%)]\tLoss: 0.322421\n",
      "Train Epoch: 3 [22784/25000 (91%)]\tLoss: 0.317187\n",
      "\n",
      "Test set: Average loss: 0.3621, Accuracy: 21077/25000 (84%)\n",
      "acc is: 0.8431, best acc is 0.8431\n",
      "\n",
      "Train Epoch: 4 [2304/25000 (9%)]\tLoss: 0.240721\n",
      "Train Epoch: 4 [4864/25000 (19%)]\tLoss: 0.293444\n",
      "Train Epoch: 4 [7424/25000 (30%)]\tLoss: 0.288075\n",
      "Train Epoch: 4 [9984/25000 (40%)]\tLoss: 0.304538\n",
      "Train Epoch: 4 [12544/25000 (50%)]\tLoss: 0.337564\n",
      "Train Epoch: 4 [15104/25000 (60%)]\tLoss: 0.263022\n",
      "Train Epoch: 4 [17664/25000 (70%)]\tLoss: 0.257990\n",
      "Train Epoch: 4 [20224/25000 (81%)]\tLoss: 0.285002\n",
      "Train Epoch: 4 [22784/25000 (91%)]\tLoss: 0.236363\n",
      "\n",
      "Test set: Average loss: 0.3312, Accuracy: 21453/25000 (86%)\n",
      "acc is: 0.8581, best acc is 0.8581\n",
      "\n",
      "Train Epoch: 5 [2304/25000 (9%)]\tLoss: 0.313676\n",
      "Train Epoch: 5 [4864/25000 (19%)]\tLoss: 0.311357\n",
      "Train Epoch: 5 [7424/25000 (30%)]\tLoss: 0.272782\n",
      "Train Epoch: 5 [9984/25000 (40%)]\tLoss: 0.270633\n",
      "Train Epoch: 5 [12544/25000 (50%)]\tLoss: 0.227613\n",
      "Train Epoch: 5 [15104/25000 (60%)]\tLoss: 0.236381\n",
      "Train Epoch: 5 [17664/25000 (70%)]\tLoss: 0.256101\n",
      "Train Epoch: 5 [20224/25000 (81%)]\tLoss: 0.208202\n",
      "Train Epoch: 5 [22784/25000 (91%)]\tLoss: 0.225739\n",
      "\n",
      "Test set: Average loss: 0.3228, Accuracy: 21649/25000 (87%)\n",
      "acc is: 0.8660, best acc is 0.8660\n",
      "\n",
      "Train Epoch: 6 [2304/25000 (9%)]\tLoss: 0.280991\n",
      "Train Epoch: 6 [4864/25000 (19%)]\tLoss: 0.360462\n",
      "Train Epoch: 6 [7424/25000 (30%)]\tLoss: 0.243861\n",
      "Train Epoch: 6 [9984/25000 (40%)]\tLoss: 0.167569\n",
      "Train Epoch: 6 [12544/25000 (50%)]\tLoss: 0.207544\n",
      "Train Epoch: 6 [15104/25000 (60%)]\tLoss: 0.268867\n",
      "Train Epoch: 6 [17664/25000 (70%)]\tLoss: 0.198942\n",
      "Train Epoch: 6 [20224/25000 (81%)]\tLoss: 0.205791\n",
      "Train Epoch: 6 [22784/25000 (91%)]\tLoss: 0.209185\n",
      "\n",
      "Test set: Average loss: 0.3516, Accuracy: 21518/25000 (86%)\n",
      "acc is: 0.8607, best acc is 0.8660\n",
      "\n",
      "Train Epoch: 7 [2304/25000 (9%)]\tLoss: 0.150860\n",
      "Train Epoch: 7 [4864/25000 (19%)]\tLoss: 0.184859\n",
      "Train Epoch: 7 [7424/25000 (30%)]\tLoss: 0.162245\n",
      "Train Epoch: 7 [9984/25000 (40%)]\tLoss: 0.168243\n",
      "Train Epoch: 7 [12544/25000 (50%)]\tLoss: 0.152089\n",
      "Train Epoch: 7 [15104/25000 (60%)]\tLoss: 0.268652\n",
      "Train Epoch: 7 [17664/25000 (70%)]\tLoss: 0.196271\n",
      "Train Epoch: 7 [20224/25000 (81%)]\tLoss: 0.189101\n",
      "Train Epoch: 7 [22784/25000 (91%)]\tLoss: 0.246779\n",
      "\n",
      "Test set: Average loss: 0.3290, Accuracy: 21720/25000 (87%)\n",
      "acc is: 0.8688, best acc is 0.8688\n",
      "\n",
      "Train Epoch: 8 [2304/25000 (9%)]\tLoss: 0.179519\n",
      "Train Epoch: 8 [4864/25000 (19%)]\tLoss: 0.151517\n",
      "Train Epoch: 8 [7424/25000 (30%)]\tLoss: 0.150295\n",
      "Train Epoch: 8 [9984/25000 (40%)]\tLoss: 0.189991\n",
      "Train Epoch: 8 [12544/25000 (50%)]\tLoss: 0.138876\n",
      "Train Epoch: 8 [15104/25000 (60%)]\tLoss: 0.158861\n",
      "Train Epoch: 8 [17664/25000 (70%)]\tLoss: 0.181690\n",
      "Train Epoch: 8 [20224/25000 (81%)]\tLoss: 0.152526\n",
      "Train Epoch: 8 [22784/25000 (91%)]\tLoss: 0.152365\n",
      "\n",
      "Test set: Average loss: 0.3832, Accuracy: 21580/25000 (86%)\n",
      "acc is: 0.8632, best acc is 0.8688\n",
      "\n",
      "Train Epoch: 9 [2304/25000 (9%)]\tLoss: 0.100630\n",
      "Train Epoch: 9 [4864/25000 (19%)]\tLoss: 0.161287\n",
      "Train Epoch: 9 [7424/25000 (30%)]\tLoss: 0.129885\n",
      "Train Epoch: 9 [9984/25000 (40%)]\tLoss: 0.181259\n",
      "Train Epoch: 9 [12544/25000 (50%)]\tLoss: 0.134567\n",
      "Train Epoch: 9 [15104/25000 (60%)]\tLoss: 0.125892\n",
      "Train Epoch: 9 [17664/25000 (70%)]\tLoss: 0.142443\n",
      "Train Epoch: 9 [20224/25000 (81%)]\tLoss: 0.185459\n",
      "Train Epoch: 9 [22784/25000 (91%)]\tLoss: 0.166584\n",
      "\n",
      "Test set: Average loss: 0.3455, Accuracy: 21712/25000 (87%)\n",
      "acc is: 0.8685, best acc is 0.8688\n",
      "\n",
      "Train Epoch: 10 [2304/25000 (9%)]\tLoss: 0.098326\n",
      "Train Epoch: 10 [4864/25000 (19%)]\tLoss: 0.129184\n",
      "Train Epoch: 10 [7424/25000 (30%)]\tLoss: 0.121770\n",
      "Train Epoch: 10 [9984/25000 (40%)]\tLoss: 0.171878\n",
      "Train Epoch: 10 [12544/25000 (50%)]\tLoss: 0.095348\n",
      "Train Epoch: 10 [15104/25000 (60%)]\tLoss: 0.126669\n",
      "Train Epoch: 10 [17664/25000 (70%)]\tLoss: 0.129144\n",
      "Train Epoch: 10 [20224/25000 (81%)]\tLoss: 0.100308\n",
      "Train Epoch: 10 [22784/25000 (91%)]\tLoss: 0.160932\n",
      "\n",
      "Test set: Average loss: 0.4487, Accuracy: 21590/25000 (86%)\n",
      "acc is: 0.8636, best acc is 0.8688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Model(MAX_WORDS, EMB_SIZE, HID_SIZE, DROPOUT).to(DEVICE)\n",
    "print(model)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "best_acc = 0.0 \n",
    "PATH = 'imdb model/model.pth'  # 定义模型保存路径\n",
    "\n",
    "for epoch in range(1, 11):  # 10个epoch\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    acc = test(model, DEVICE, test_loader)\n",
    "    if best_acc < acc: \n",
    "        best_acc = acc \n",
    "        torch.save(model.state_dict(), PATH)\n",
    "    print(\"acc is: {:.4f}, best acc is {:.4f}\\n\".format(acc, best_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3290, Accuracy: 21720/25000 (87%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8688"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检验保存的模型\n",
    "best_model = Model(MAX_WORDS, EMB_SIZE, HID_SIZE, DROPOUT).to(DEVICE)\n",
    "best_model.load_state_dict(torch.load(PATH))\n",
    "test(best_model, DEVICE, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
