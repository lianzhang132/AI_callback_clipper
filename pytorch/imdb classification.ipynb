{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import * \n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.datasets import imdb \n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 10000  # imdb’s vocab_size 即词汇表大小\n",
    "MAX_LEN = 200      # max length\n",
    "BATCH_SIZE = 256\n",
    "EMB_SIZE = 128   # embedding size\n",
    "HID_SIZE = 128   # lstm hidden size\n",
    "DROPOUT = 0.2 \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 200) (25000, 200)\n"
     ]
    }
   ],
   "source": [
    "# 借助Keras加载imdb数据集\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=MAX_WORDS)\n",
    "x_train = pad_sequences(x_train, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "x_test = pad_sequences(x_test, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转化为TensorDataset\n",
    "train_data = TensorDataset(torch.LongTensor(x_train), torch.LongTensor(y_train))\n",
    "test_data = TensorDataset(torch.LongTensor(x_test), torch.LongTensor(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转化为 DataLoader\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义lstm模型用于文本分类\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, max_words, emb_size, hid_size, dropout):\n",
    "        super(Model, self).__init__()\n",
    "        self.max_words = max_words\n",
    "        self.emb_size = emb_size\n",
    "        self.hid_size = hid_size\n",
    "        self.dropout = dropout\n",
    "        self.Embedding = nn.Embedding(self.max_words, self.emb_size)\n",
    "        self.LSTM = nn.LSTM(self.emb_size, self.hid_size, num_layers=2,\n",
    "                            batch_first=True, bidirectional=True)   # 2层双向LSTM\n",
    "        self.dp = nn.Dropout(self.dropout)\n",
    "        self.fc1 = nn.Linear(self.hid_size*2, self.hid_size)\n",
    "        self.fc2 = nn.Linear(self.hid_size, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input : [bs, maxlen]\n",
    "        output: [bs, 2] \n",
    "        \"\"\"\n",
    "        x = self.Embedding(x)  # [bs, ml, emb_size]\n",
    "        x = self.dp(x)\n",
    "        x, _ = self.LSTM(x)  # [bs, ml, 2*hid_size]\n",
    "        x = self.dp(x)\n",
    "        x = F.relu(self.fc1(x))   # [bs, ml, hid_size]\n",
    "        x = F.avg_pool2d(x, (x.shape[1], 1)).squeeze()  # [bs, 1, hid_size] => [bs, hid_size]\n",
    "        out = self.fc2(x)    # [bs, 2]\n",
    "        return out  # [bs, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):   # 训练模型\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        y_ = model(x)\n",
    "        loss = criterion(y_, y)  # 得到loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(batch_idx + 1) % 10 == 0:    # 打印loss\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(x), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):    # 测试模型\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')  # 累加loss\n",
    "    test_loss = 0.0 \n",
    "    acc = 0 \n",
    "    for batch_idx, (x, y) in enumerate(test_loader):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            y_ = model(x)\n",
    "        test_loss += criterion(y_, y)\n",
    "        pred = y_.max(-1, keepdim=True)[1]   # .max() 2输出，分别为最大值和最大值的index\n",
    "        acc += pred.eq(y.view_as(pred)).sum().item()    # 记得加item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, acc, len(test_loader.dataset),\n",
    "        100. * acc / len(test_loader.dataset)))\n",
    "    return acc / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (Embedding): Embedding(10000, 128)\n",
      "  (LSTM): LSTM(128, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (dp): Dropout(p=0.2)\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "Train Epoch: 1 [2304/25000 (9%)]\tLoss: 0.681210\n",
      "Train Epoch: 1 [4864/25000 (19%)]\tLoss: 0.687217\n",
      "Train Epoch: 1 [7424/25000 (30%)]\tLoss: 0.664616\n",
      "Train Epoch: 1 [9984/25000 (40%)]\tLoss: 0.613519\n",
      "Train Epoch: 1 [12544/25000 (50%)]\tLoss: 0.580814\n",
      "Train Epoch: 1 [15104/25000 (60%)]\tLoss: 0.585284\n",
      "Train Epoch: 1 [17664/25000 (70%)]\tLoss: 0.534598\n",
      "Train Epoch: 1 [20224/25000 (81%)]\tLoss: 0.495394\n",
      "Train Epoch: 1 [22784/25000 (91%)]\tLoss: 0.473497\n",
      "\n",
      "Test set: Average loss: 0.4759, Accuracy: 19405/25000 (78%)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'imdb model/model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4ccd4fe6b78b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbest_acc\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"acc is: {:.4f}, best acc is {:.4f}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \"\"\"\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'imdb model/model.pth'"
     ]
    }
   ],
   "source": [
    "model = Model(MAX_WORDS, EMB_SIZE, HID_SIZE, DROPOUT).to(DEVICE)\n",
    "print(model)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "best_acc = 0.0 \n",
    "PATH = 'imdb model/model.pth'  # 定义模型保存路径\n",
    "\n",
    "for epoch in range(1, 11):  # 10个epoch\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    acc = test(model, DEVICE, test_loader)\n",
    "    if best_acc < acc: \n",
    "        best_acc = acc \n",
    "        torch.save(model.state_dict(), PATH)\n",
    "    print(\"acc is: {:.4f}, best acc is {:.4f}\\n\".format(acc, best_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_lstm/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.4759, Accuracy: 19405/25000 (78%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7762"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检验保存的模型\n",
    "best_model = Model(MAX_WORDS, EMB_SIZE, HID_SIZE, DROPOUT).to(DEVICE)\n",
    "best_model.load_state_dict(torch.load('model_lstm/model.pth'))\n",
    "test(best_model, DEVICE, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
