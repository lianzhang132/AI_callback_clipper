{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练...\n",
      "training on  cpu\n",
      "epoch 1, loss 0.5063, train acc 0.813, test acc 0.879, time 545.3 sec\n",
      "find best! save at model/best.pth\n",
      "epoch 2, loss 0.3155, train acc 0.885, test acc 0.894, time 543.8 sec\n",
      "find best! save at model/best.pth\n",
      "epoch 3, loss 0.2713, train acc 0.900, test acc 0.910, time 554.2 sec\n",
      "find best! save at model/best.pth\n",
      "epoch 4, loss 0.2474, train acc 0.909, test acc 0.915, time 555.7 sec\n",
      "find best! save at model/best.pth\n",
      "epoch 5, loss 0.2330, train acc 0.914, test acc 0.918, time 556.2 sec\n",
      "find best! save at model/best.pth\n",
      "epoch 6, loss 0.1841, train acc 0.933, test acc 0.930, time 567.3 sec\n",
      "find best! save at model/best.pth\n",
      "epoch 7, loss 0.1740, train acc 0.935, test acc 0.931, time 547.3 sec\n",
      "find best! save at model/best.pth\n",
      "epoch 8, loss 0.1677, train acc 0.939, test acc 0.931, time 548.3 sec\n",
      "epoch 9, loss 0.1637, train acc 0.941, test acc 0.933, time 552.4 sec\n",
      "find best! save at model/best.pth\n",
      "epoch 10, loss 0.1606, train acc 0.941, test acc 0.935, time 599.3 sec\n",
      "find best! save at model/best.pth\n",
      "epoch 11, loss 0.1551, train acc 0.944, test acc 0.936, time 718.4 sec\n",
      "find best! save at model/best.pth\n",
      "epoch 12, loss 0.1524, train acc 0.945, test acc 0.935, time 546.6 sec\n",
      "epoch 13, loss 0.1507, train acc 0.945, test acc 0.935, time 535.3 sec\n",
      "epoch 14, loss 0.1505, train acc 0.945, test acc 0.935, time 551.4 sec\n",
      "epoch 15, loss 0.1489, train acc 0.945, test acc 0.936, time 542.7 sec\n",
      "epoch 16, loss 0.1498, train acc 0.946, test acc 0.935, time 548.9 sec\n",
      "epoch 17, loss 0.1479, train acc 0.947, test acc 0.936, time 548.7 sec\n",
      "epoch 18, loss 0.1483, train acc 0.946, test acc 0.935, time 551.8 sec\n",
      "epoch 19, loss 0.1477, train acc 0.946, test acc 0.935, time 557.5 sec\n",
      "epoch 20, loss 0.1470, train acc 0.947, test acc 0.935, time 553.7 sec\n",
      "epoch 21, loss 0.1474, train acc 0.947, test acc 0.935, time 541.5 sec\n",
      "epoch 22, loss 0.1478, train acc 0.947, test acc 0.936, time 544.2 sec\n",
      "find best! save at model/best.pth\n",
      "epoch 23, loss 0.1490, train acc 0.945, test acc 0.935, time 546.1 sec\n",
      "epoch 24, loss 0.1492, train acc 0.946, test acc 0.935, time 544.0 sec\n",
      "epoch 25, loss 0.1483, train acc 0.946, test acc 0.935, time 547.6 sec\n",
      "epoch 26, loss 0.1479, train acc 0.947, test acc 0.936, time 546.9 sec\n",
      "epoch 27, loss 0.1488, train acc 0.945, test acc 0.935, time 542.7 sec\n",
      "epoch 28, loss 0.1474, train acc 0.946, test acc 0.935, time 547.1 sec\n",
      "epoch 29, loss 0.1492, train acc 0.945, test acc 0.935, time 546.5 sec\n",
      "epoch 30, loss 0.1464, train acc 0.947, test acc 0.936, time 553.2 sec\n",
      "find best! save at model/best.pth\n",
      "epoch 31, loss 0.1503, train acc 0.945, test acc 0.935, time 544.6 sec\n",
      "epoch 32, loss 0.1479, train acc 0.947, test acc 0.936, time 543.1 sec\n",
      "epoch 33, loss 0.1483, train acc 0.946, test acc 0.936, time 565.2 sec\n",
      "find best! save at model/best.pth\n",
      "epoch 34, loss 0.1491, train acc 0.946, test acc 0.936, time 545.0 sec\n",
      "epoch 35, loss 0.1489, train acc 0.946, test acc 0.935, time 543.7 sec\n",
      "epoch 36, loss 0.1489, train acc 0.946, test acc 0.934, time 542.6 sec\n",
      "epoch 37, loss 0.1485, train acc 0.945, test acc 0.934, time 546.5 sec\n",
      "epoch 38, loss 0.1484, train acc 0.946, test acc 0.935, time 546.0 sec\n",
      "epoch 39, loss 0.1472, train acc 0.946, test acc 0.936, time 547.3 sec\n",
      "epoch 40, loss 0.1490, train acc 0.946, test acc 0.935, time 544.0 sec\n",
      "epoch 41, loss 0.1484, train acc 0.946, test acc 0.936, time 554.8 sec\n",
      "epoch 42, loss 0.1492, train acc 0.946, test acc 0.935, time 548.5 sec\n",
      "epoch 43, loss 0.1486, train acc 0.945, test acc 0.935, time 544.9 sec\n",
      "epoch 44, loss 0.1475, train acc 0.947, test acc 0.935, time 547.8 sec\n",
      "epoch 45, loss 0.1493, train acc 0.946, test acc 0.936, time 547.2 sec\n",
      "epoch 46, loss 0.1490, train acc 0.946, test acc 0.935, time 545.6 sec\n",
      "epoch 47, loss 0.1480, train acc 0.947, test acc 0.935, time 549.4 sec\n",
      "epoch 48, loss 0.1485, train acc 0.946, test acc 0.935, time 542.9 sec\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    \"\"\"\n",
    "    全局平均池化层\n",
    "    可通过将普通的平均池化的窗口形状设置成输入的高和宽实现\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "\n",
    "\n",
    "class FlattenLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):  # x shape: (batch, *, *, ...)\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1):\n",
    "        \"\"\"\n",
    "            use_1×1conv: 是否使用额外的1x1卷积层来修改通道数\n",
    "            stride: 卷积层的步幅, resnet使用步长为2的卷积来替代pooling的作用，是个很赞的idea\n",
    "        \"\"\"\n",
    "        super(Residual, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        return F.relu(Y + X)\n",
    "\n",
    "\n",
    "def resnet_block(in_channels, out_channels, num_residuals, first_block=False):\n",
    "    '''\n",
    "    resnet block\n",
    "    num_residuals: 当前block包含多少个残差块\n",
    "    first_block: 是否为第一个block\n",
    "    一个resnet block由num_residuals个残差块组成\n",
    "    其中第一个残差块起到了通道数的转换和pooling的作用\n",
    "    后面的若干残差块就是完成正常的特征提取\n",
    "    '''\n",
    "    if first_block:\n",
    "        assert in_channels == out_channels  # 第一个模块的输出通道数同输入通道数一致\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(in_channels, out_channels, use_1x1conv=True, stride=2))\n",
    "        else:\n",
    "            blk.append(Residual(out_channels, out_channels))\n",
    "    return nn.Sequential(*blk)\n",
    "\n",
    "\n",
    "# 定义resnet模型结构\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # TODO: 缩小感受野, 缩channel\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU())\n",
    "# nn.ReLU(),\n",
    "# nn.MaxPool2d(kernel_size=2, stride=2))   # TODO：去掉maxpool缩小感受野\n",
    "\n",
    "# 然后是连续4个block\n",
    "net.add_module(\"resnet_block1\", resnet_block(32, 32, 2, first_block=True))  # TODO: channel统一减半\n",
    "net.add_module(\"resnet_block2\", resnet_block(32, 64, 2))\n",
    "net.add_module(\"resnet_block3\", resnet_block(64, 128, 2))\n",
    "net.add_module(\"resnet_block4\", resnet_block(128, 256, 2))\n",
    "# global average pooling\n",
    "net.add_module(\"global_avg_pool\", GlobalAvgPool2d())\n",
    "# fc layer\n",
    "net.add_module(\"fc\", nn.Sequential(FlattenLayer(), nn.Linear(256, 10)))\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, root='../data'):\n",
    "    \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.28], std=[0.35])\n",
    "    train_augs = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding=2),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    test_augs = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=train_augs)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=test_augs)\n",
    "    if sys.platform.startswith('win'):\n",
    "        num_workers = 0  # 0表示不用额外的进程来加速读取数据\n",
    "    else:\n",
    "        num_workers = 4\n",
    "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_iter, test_iter\n",
    "\n",
    "\n",
    "print('训练...')\n",
    "batch_size = 64\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, root='/root/.pytorch/F_MNIST_data')\n",
    "\n",
    "\n",
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        # 如果没指定device就使用net的device\n",
    "        device = list(net.parameters())[0].device\n",
    "    net.eval()\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "    net.train()  # 改回训练模式\n",
    "    return acc_sum / n\n",
    "\n",
    "\n",
    "def train_model(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs, lr, lr_period, lr_decay):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    best_test_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\n",
    "\n",
    "        if epoch > 0 and epoch % lr_period == 0:  # 每lr_period个epoch，学习率衰减一次\n",
    "            lr = lr * lr_decay\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\n",
    "        if test_acc > best_test_acc:\n",
    "            print('find best! save at model/best.pth')\n",
    "            best_test_acc = test_acc\n",
    "            torch.save(net.state_dict(), 'model/best.pth')\n",
    "            # utils.save_model({\n",
    "            #    'arch': args.model,\n",
    "            #    'state_dict': net.state_dict()\n",
    "            # }, 'saved-models/{}-run-{}.pth.tar'.format(args.model, run))\n",
    "\n",
    "\n",
    "lr, num_epochs, lr_period, lr_decay = 0.01, 50, 5, 0.1\n",
    "#optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_model(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs, lr, lr_period, lr_decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('加载最优模型')\n",
    "net.load_state_dict(torch.load('model/best.pth'))\n",
    "net = net.to(device)\n",
    "\n",
    "print('inference测试集')\n",
    "net.eval()\n",
    "id = 0\n",
    "preds_list = []\n",
    "with torch.no_grad():\n",
    "    for X, y in test_iter:\n",
    "        batch_pred = list(net(X.to(device)).argmax(dim=1).cpu().numpy())\n",
    "        for y_pred in batch_pred:\n",
    "            preds_list.append((id, y_pred))\n",
    "            id += 1\n",
    "\n",
    "print('生成测试集评估文件')\n",
    "with open('result.csv', 'w') as f:\n",
    "    f.write('ID,Prediction\\n')\n",
    "    for id, pred in preds_list:\n",
    "        f.write('{},{}\\n'.format(id, pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
