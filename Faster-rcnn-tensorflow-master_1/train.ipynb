{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/network.py:28: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/network.py:104: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tfplot/ops.py:114: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/network.py:192: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/network.py:64: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/network.py:247: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/network.py:266: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/losslayer.py:26: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Loading gt_labels from: annotation_cache/VOC2007_trainval/pascal_train_gt_labels.pkl\n",
      "Appending horizontally-flipped training examples ...\n",
      "Loading gt_labels from: annotation_cache/VOC2007_test/pascal_test_gt_labels.pkl\n",
      "start training\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/losslayer.py:99: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "reg\n",
      "Tensor(\"Sum:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/losslayer.py:120: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "vgg_16/conv1/conv1_1/weights:0\n",
      "vgg_16/conv1/conv1_1/biases:0\n",
      "vgg_16/conv1/conv1_2/weights:0\n",
      "vgg_16/conv1/conv1_2/biases:0\n",
      "vgg_16/conv2/conv2_1/weights:0\n",
      "vgg_16/conv2/conv2_1/biases:0\n",
      "vgg_16/conv2/conv2_2/weights:0\n",
      "vgg_16/conv2/conv2_2/biases:0\n",
      "vgg_16/conv3/conv3_1/weights:0\n",
      "vgg_16/conv3/conv3_1/biases:0\n",
      "vgg_16/conv3/conv3_2/weights:0\n",
      "vgg_16/conv3/conv3_2/biases:0\n",
      "vgg_16/conv3/conv3_3/weights:0\n",
      "vgg_16/conv3/conv3_3/biases:0\n",
      "vgg_16/conv4/conv4_1/weights:0\n",
      "vgg_16/conv4/conv4_1/biases:0\n",
      "vgg_16/conv4/conv4_2/weights:0\n",
      "vgg_16/conv4/conv4_2/biases:0\n",
      "vgg_16/conv4/conv4_3/weights:0\n",
      "vgg_16/conv4/conv4_3/biases:0\n",
      "vgg_16/conv5/conv5_1/weights:0\n",
      "vgg_16/conv5/conv5_1/biases:0\n",
      "vgg_16/conv5/conv5_2/weights:0\n",
      "vgg_16/conv5/conv5_2/biases:0\n",
      "vgg_16/conv5/conv5_3/weights:0\n",
      "vgg_16/conv5/conv5_3/biases:0\n",
      "rpn/conv6/weights:0\n",
      "rpn/conv6/biases:0\n",
      "rpn/conv7/weights:0\n",
      "rpn/conv7/biases:0\n",
      "rpn/conv8/weights:0\n",
      "rpn/conv8/biases:0\n",
      "vgg_16/fc6/weights:0\n",
      "vgg_16/fc6/biases:0\n",
      "vgg_16/fc7/weights:0\n",
      "vgg_16/fc7/biases:0\n",
      "vgg_16/region_deciton/cls_score/weights:0\n",
      "vgg_16/region_deciton/cls_score/biases:0\n",
      "vgg_16/region_deciton/bbox_pred/weights:0\n",
      "vgg_16/region_deciton/bbox_pred/biases:0\n",
      "Variable:0\n",
      "vgg_16/conv3/conv3_1/weights/Momentum:0\n",
      "vgg_16/conv3/conv3_1/biases/Momentum:0\n",
      "vgg_16/conv3/conv3_2/weights/Momentum:0\n",
      "vgg_16/conv3/conv3_2/biases/Momentum:0\n",
      "vgg_16/conv3/conv3_3/weights/Momentum:0\n",
      "vgg_16/conv3/conv3_3/biases/Momentum:0\n",
      "vgg_16/conv4/conv4_1/weights/Momentum:0\n",
      "vgg_16/conv4/conv4_1/biases/Momentum:0\n",
      "vgg_16/conv4/conv4_2/weights/Momentum:0\n",
      "vgg_16/conv4/conv4_2/biases/Momentum:0\n",
      "vgg_16/conv4/conv4_3/weights/Momentum:0\n",
      "vgg_16/conv4/conv4_3/biases/Momentum:0\n",
      "vgg_16/conv5/conv5_1/weights/Momentum:0\n",
      "vgg_16/conv5/conv5_1/biases/Momentum:0\n",
      "vgg_16/conv5/conv5_2/weights/Momentum:0\n",
      "vgg_16/conv5/conv5_2/biases/Momentum:0\n",
      "vgg_16/conv5/conv5_3/weights/Momentum:0\n",
      "vgg_16/conv5/conv5_3/biases/Momentum:0\n",
      "rpn/conv6/weights/Momentum:0\n",
      "rpn/conv6/biases/Momentum:0\n",
      "rpn/conv7/weights/Momentum:0\n",
      "rpn/conv7/biases/Momentum:0\n",
      "rpn/conv8/weights/Momentum:0\n",
      "rpn/conv8/biases/Momentum:0\n",
      "vgg_16/fc6/weights/Momentum:0\n",
      "vgg_16/fc6/biases/Momentum:0\n",
      "vgg_16/fc7/weights/Momentum:0\n",
      "vgg_16/fc7/biases/Momentum:0\n",
      "vgg_16/region_deciton/cls_score/weights/Momentum:0\n",
      "vgg_16/region_deciton/cls_score/biases/Momentum:0\n",
      "vgg_16/region_deciton/bbox_pred/weights/Momentum:0\n",
      "vgg_16/region_deciton/bbox_pred/biases/Momentum:0\n",
      "INFO:tensorflow:Restoring parameters from model_pretrained/vgg_16.ckpt\n",
      "Fix VGG16 layers..\n",
      "INFO:tensorflow:Restoring parameters from model_pretrained/vgg_16.ckpt\n",
      "The 0 step train_total_loss is 3.767681 val_total_loss is 6.8167686\n",
      "learning_rate is  0.001\n",
      "The 50 step train_total_loss is 0.110117465 val_total_loss is 0.14852998\n",
      "learning_rate is  0.001\n",
      "The 100 step train_total_loss is 0.17914693 val_total_loss is 0.10573257\n",
      "learning_rate is  0.001\n",
      "The 150 step train_total_loss is 0.11849172 val_total_loss is 0.17537075\n",
      "learning_rate is  0.001\n",
      "The 200 step train_total_loss is 0.62919927 val_total_loss is 0.39321423\n",
      "learning_rate is  0.001\n",
      "The 250 step train_total_loss is 0.3171407 val_total_loss is 0.037850395\n",
      "learning_rate is  0.001\n",
      "The 300 step train_total_loss is 0.18699819 val_total_loss is 0.18127726\n",
      "learning_rate is  0.001\n",
      "The 350 step train_total_loss is 0.06556687 val_total_loss is 0.025647055\n",
      "learning_rate is  0.001\n",
      "The 400 step train_total_loss is 0.12732282 val_total_loss is 0.07248164\n",
      "learning_rate is  0.001\n",
      "The 450 step train_total_loss is 0.13754691 val_total_loss is 0.25348788\n",
      "learning_rate is  0.001\n",
      "The 500 step train_total_loss is 0.8484038 val_total_loss is 0.84876865\n",
      "learning_rate is  0.001\n",
      "The 550 step train_total_loss is 0.41863766 val_total_loss is 0.6166923\n",
      "learning_rate is  0.001\n",
      "The 600 step train_total_loss is 0.14376311 val_total_loss is 0.09227078\n",
      "learning_rate is  0.001\n",
      "The 650 step train_total_loss is 0.10510337 val_total_loss is 0.7500905\n",
      "learning_rate is  0.001\n",
      "The 700 step train_total_loss is 0.25614396 val_total_loss is 0.18779808\n",
      "learning_rate is  0.001\n",
      "The 750 step train_total_loss is 0.30565265 val_total_loss is 0.24185723\n",
      "learning_rate is  0.001\n",
      "The 800 step train_total_loss is 0.10416654 val_total_loss is 0.13102508\n",
      "learning_rate is  0.001\n",
      "The 850 step train_total_loss is 0.059831806 val_total_loss is 0.26179868\n",
      "learning_rate is  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 900 step train_total_loss is 0.7260959 val_total_loss is 0.5231265\n",
      "learning_rate is  0.001\n",
      "The 950 step train_total_loss is 0.17131145 val_total_loss is 0.52775455\n",
      "learning_rate is  0.001\n",
      "The 1000 step train_total_loss is 0.42995507 val_total_loss is 0.41908002\n",
      "learning_rate is  0.001\n",
      "The 1050 step train_total_loss is 0.58942294 val_total_loss is 0.3054273\n",
      "learning_rate is  0.001\n",
      "The 1100 step train_total_loss is 0.16183375 val_total_loss is 0.16329779\n",
      "learning_rate is  0.001\n",
      "The 1150 step train_total_loss is 0.04562131 val_total_loss is 0.5633826\n",
      "learning_rate is  0.001\n",
      "The 1200 step train_total_loss is 1.1555414 val_total_loss is 0.76915216\n",
      "learning_rate is  0.001\n",
      "The 1250 step train_total_loss is 0.30301613 val_total_loss is 0.5144563\n",
      "learning_rate is  0.001\n",
      "The 1300 step train_total_loss is 0.19840613 val_total_loss is 1.0491308\n",
      "learning_rate is  0.001\n",
      "The 1350 step train_total_loss is 0.76914036 val_total_loss is 0.6052792\n",
      "learning_rate is  0.001\n",
      "The 1400 step train_total_loss is 0.48444408 val_total_loss is 0.19328472\n",
      "learning_rate is  0.001\n",
      "The 1450 step train_total_loss is 1.1073258 val_total_loss is 0.55671066\n",
      "learning_rate is  0.001\n",
      "The 1500 step train_total_loss is 0.17907538 val_total_loss is 0.36825663\n",
      "learning_rate is  0.001\n",
      "The 1550 step train_total_loss is 0.5259636 val_total_loss is 0.64156663\n",
      "learning_rate is  0.001\n",
      "The 1600 step train_total_loss is 0.43588614 val_total_loss is 0.25785658\n",
      "learning_rate is  0.001\n",
      "The 1650 step train_total_loss is 0.2515739 val_total_loss is 0.3904357\n",
      "learning_rate is  0.001\n",
      "The 1700 step train_total_loss is 0.6143558 val_total_loss is 0.14933932\n",
      "learning_rate is  0.001\n",
      "The 1750 step train_total_loss is 0.5616712 val_total_loss is 0.24098814\n",
      "learning_rate is  0.001\n",
      "The 1800 step train_total_loss is 1.5765848 val_total_loss is 0.40629327\n",
      "learning_rate is  0.001\n",
      "The 1850 step train_total_loss is 0.8424414 val_total_loss is 0.37242335\n",
      "learning_rate is  0.001\n",
      "The 1900 step train_total_loss is 0.07567065 val_total_loss is 0.066046685\n",
      "learning_rate is  0.001\n",
      "The 1950 step train_total_loss is 1.0580325 val_total_loss is 1.0941963\n",
      "learning_rate is  0.001\n",
      "The 2000 step train_total_loss is 0.55975956 val_total_loss is 0.45025918\n",
      "learning_rate is  0.001\n",
      "The 2050 step train_total_loss is 0.39870232 val_total_loss is 0.2803525\n",
      "learning_rate is  0.001\n",
      "The 2100 step train_total_loss is 0.048644833 val_total_loss is 0.5821989\n",
      "learning_rate is  0.001\n",
      "The 2150 step train_total_loss is 1.0069872 val_total_loss is 1.1457871\n",
      "learning_rate is  0.001\n",
      "The 2200 step train_total_loss is 0.15354943 val_total_loss is 0.7170492\n",
      "learning_rate is  0.001\n",
      "The 2250 step train_total_loss is 0.37992662 val_total_loss is 0.3151664\n",
      "learning_rate is  0.001\n",
      "The 2300 step train_total_loss is 0.08060911 val_total_loss is 0.0729657\n",
      "learning_rate is  0.001\n",
      "The 2350 step train_total_loss is 0.27783757 val_total_loss is 0.40230367\n",
      "learning_rate is  0.001\n",
      "The 2400 step train_total_loss is 0.22249457 val_total_loss is 0.45026046\n",
      "learning_rate is  0.001\n",
      "The 2450 step train_total_loss is 0.24392506 val_total_loss is 0.6062532\n",
      "learning_rate is  0.001\n",
      "The 2500 step train_total_loss is 0.27301046 val_total_loss is 0.2168878\n",
      "learning_rate is  0.001\n",
      "The 2550 step train_total_loss is 0.6503783 val_total_loss is 0.44147086\n",
      "learning_rate is  0.001\n",
      "The 2600 step train_total_loss is 0.2877418 val_total_loss is 0.4769712\n",
      "learning_rate is  0.001\n",
      "The 2650 step train_total_loss is 0.30196825 val_total_loss is 0.3436178\n",
      "learning_rate is  0.001\n",
      "The 2700 step train_total_loss is 0.09898876 val_total_loss is 0.11541816\n",
      "learning_rate is  0.001\n",
      "The 2750 step train_total_loss is 0.41591883 val_total_loss is 0.14362033\n",
      "learning_rate is  0.001\n",
      "The 2800 step train_total_loss is 0.7613366 val_total_loss is 0.63064253\n",
      "learning_rate is  0.001\n",
      "The 2850 step train_total_loss is 0.64356816 val_total_loss is 0.25482148\n",
      "learning_rate is  0.001\n",
      "The 2900 step train_total_loss is 0.088324755 val_total_loss is 0.18075989\n",
      "learning_rate is  0.001\n",
      "The 2950 step train_total_loss is 0.28188723 val_total_loss is 0.45413113\n",
      "learning_rate is  0.001\n",
      "The 3000 step train_total_loss is 0.23979855 val_total_loss is 0.20167084\n",
      "learning_rate is  0.001\n",
      "The 3050 step train_total_loss is 0.9594314 val_total_loss is 0.15332575\n",
      "learning_rate is  0.001\n",
      "The 3100 step train_total_loss is 0.90100896 val_total_loss is 0.529998\n",
      "learning_rate is  0.001\n",
      "The 3150 step train_total_loss is 0.108812556 val_total_loss is 0.023708045\n",
      "learning_rate is  0.001\n",
      "The 3200 step train_total_loss is 0.37372366 val_total_loss is 0.33452663\n",
      "learning_rate is  0.001\n",
      "The 3250 step train_total_loss is 0.43464053 val_total_loss is 0.35098642\n",
      "learning_rate is  0.001\n",
      "The 3300 step train_total_loss is 0.19975865 val_total_loss is 0.38936955\n",
      "learning_rate is  0.001\n",
      "The 3350 step train_total_loss is 0.22808234 val_total_loss is 0.12991539\n",
      "learning_rate is  0.001\n",
      "The 3400 step train_total_loss is 1.453085 val_total_loss is 0.1199563\n",
      "learning_rate is  0.001\n",
      "The 3450 step train_total_loss is 0.68401814 val_total_loss is 0.2064302\n",
      "learning_rate is  0.001\n",
      "The 3500 step train_total_loss is 0.36249077 val_total_loss is 0.2665545\n",
      "learning_rate is  0.001\n",
      "The 3550 step train_total_loss is 0.07488491 val_total_loss is 0.33828485\n",
      "learning_rate is  0.001\n",
      "The 3600 step train_total_loss is 0.17469841 val_total_loss is 0.36948466\n",
      "learning_rate is  0.001\n",
      "The 3650 step train_total_loss is 0.74922913 val_total_loss is 0.47163588\n",
      "learning_rate is  0.001\n",
      "The 3700 step train_total_loss is 0.2953201 val_total_loss is 0.18136165\n",
      "learning_rate is  0.001\n",
      "The 3750 step train_total_loss is 0.30234376 val_total_loss is 0.27077165\n",
      "learning_rate is  0.001\n",
      "The 3800 step train_total_loss is 0.48446238 val_total_loss is 0.5407665\n",
      "learning_rate is  0.001\n",
      "The 3850 step train_total_loss is 0.92052835 val_total_loss is 0.30605492\n",
      "learning_rate is  0.001\n",
      "The 3900 step train_total_loss is 0.27058196 val_total_loss is 1.4744376\n",
      "learning_rate is  0.001\n",
      "The 3950 step train_total_loss is 0.6280689 val_total_loss is 0.512808\n",
      "learning_rate is  0.001\n",
      "The 4000 step train_total_loss is 0.32673734 val_total_loss is 0.29136944\n",
      "learning_rate is  0.001\n",
      "The 4050 step train_total_loss is 0.90930927 val_total_loss is 0.61071604\n",
      "learning_rate is  0.001\n",
      "The 4100 step train_total_loss is 1.0238844 val_total_loss is 0.17549331\n",
      "learning_rate is  0.001\n",
      "The 4150 step train_total_loss is 0.35957247 val_total_loss is 0.2504886\n",
      "learning_rate is  0.001\n",
      "The 4200 step train_total_loss is 0.23990658 val_total_loss is 0.12596211\n",
      "learning_rate is  0.001\n",
      "The 4250 step train_total_loss is 0.12908956 val_total_loss is 0.4314595\n",
      "learning_rate is  0.001\n",
      "The 4300 step train_total_loss is 0.21416874 val_total_loss is 0.43930906\n",
      "learning_rate is  0.001\n",
      "The 4350 step train_total_loss is 1.1255538 val_total_loss is 1.5238308\n",
      "learning_rate is  0.001\n",
      "The 4400 step train_total_loss is 0.48537412 val_total_loss is 0.66667974\n",
      "learning_rate is  0.001\n",
      "The 4450 step train_total_loss is 0.078850396 val_total_loss is 1.7179503\n",
      "learning_rate is  0.001\n",
      "The 4500 step train_total_loss is 0.27943414 val_total_loss is 0.39436355\n",
      "learning_rate is  0.001\n",
      "The 4550 step train_total_loss is 1.1649402 val_total_loss is 0.32514635\n",
      "learning_rate is  0.001\n",
      "The 4600 step train_total_loss is 1.6017513 val_total_loss is 0.5055494\n",
      "learning_rate is  0.001\n",
      "The 4650 step train_total_loss is 0.24062411 val_total_loss is 0.13511878\n",
      "learning_rate is  0.001\n",
      "The 4700 step train_total_loss is 0.10229659 val_total_loss is 0.8169221\n",
      "learning_rate is  0.001\n",
      "The 4750 step train_total_loss is 0.24508317 val_total_loss is 0.63200235\n",
      "learning_rate is  0.001\n",
      "The 4800 step train_total_loss is 0.16489886 val_total_loss is 0.35252035\n",
      "learning_rate is  0.001\n",
      "The 4850 step train_total_loss is 0.87460357 val_total_loss is 0.1328785\n",
      "learning_rate is  0.001\n",
      "The 4900 step train_total_loss is 0.22844827 val_total_loss is 0.14950505\n",
      "learning_rate is  0.001\n",
      "The 4950 step train_total_loss is 0.33497006 val_total_loss is 0.4176878\n",
      "learning_rate is  0.001\n",
      "The 5000 step train_total_loss is 0.3544683 val_total_loss is 1.1174979\n",
      "learning_rate is  0.001\n",
      "The 5050 step train_total_loss is 1.533304 val_total_loss is 0.56722057\n",
      "learning_rate is  0.001\n",
      "The 5100 step train_total_loss is 0.9473731 val_total_loss is 0.09230348\n",
      "learning_rate is  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5150 step train_total_loss is 0.47906002 val_total_loss is 0.7076856\n",
      "learning_rate is  0.001\n",
      "The 5200 step train_total_loss is 0.2790257 val_total_loss is 0.1587047\n",
      "learning_rate is  0.001\n",
      "The 5250 step train_total_loss is 0.18186711 val_total_loss is 1.1643443\n",
      "learning_rate is  0.001\n",
      "The 5300 step train_total_loss is 0.22621053 val_total_loss is 0.461559\n",
      "learning_rate is  0.001\n",
      "The 5350 step train_total_loss is 0.30772755 val_total_loss is 0.12262987\n",
      "learning_rate is  0.001\n",
      "The 5400 step train_total_loss is 1.0604599 val_total_loss is 1.0490677\n",
      "learning_rate is  0.001\n",
      "The 5450 step train_total_loss is 0.6234527 val_total_loss is 0.24255753\n",
      "learning_rate is  0.001\n",
      "The 5500 step train_total_loss is 0.492837 val_total_loss is 1.9687893\n",
      "learning_rate is  0.001\n",
      "The 5550 step train_total_loss is 0.7544813 val_total_loss is 0.40978765\n",
      "learning_rate is  0.001\n",
      "The 5600 step train_total_loss is 0.5450933 val_total_loss is 0.3365354\n",
      "learning_rate is  0.001\n",
      "The 5650 step train_total_loss is 0.28271535 val_total_loss is 0.2249845\n",
      "learning_rate is  0.001\n",
      "The 5700 step train_total_loss is 0.88207614 val_total_loss is 0.6897727\n",
      "learning_rate is  0.001\n",
      "The 5750 step train_total_loss is 1.0850295 val_total_loss is 0.44417298\n",
      "learning_rate is  0.001\n",
      "The 5800 step train_total_loss is 0.5271136 val_total_loss is 0.40074977\n",
      "learning_rate is  0.001\n",
      "The 5850 step train_total_loss is 0.4362836 val_total_loss is 0.40452972\n",
      "learning_rate is  0.001\n",
      "The 5900 step train_total_loss is 0.18718725 val_total_loss is 0.34064215\n",
      "learning_rate is  0.001\n",
      "The 5950 step train_total_loss is 0.06345266 val_total_loss is 0.20782799\n",
      "learning_rate is  0.001\n",
      "The 6000 step train_total_loss is 0.18003973 val_total_loss is 0.53067833\n",
      "learning_rate is  0.001\n",
      "The 6050 step train_total_loss is 0.9219617 val_total_loss is 0.2745485\n",
      "learning_rate is  0.001\n",
      "The 6100 step train_total_loss is 0.6525868 val_total_loss is 0.3315108\n",
      "learning_rate is  0.001\n",
      "The 6150 step train_total_loss is 0.06903751 val_total_loss is 0.12205755\n",
      "learning_rate is  0.001\n",
      "The 6200 step train_total_loss is 0.12011693 val_total_loss is 0.69778603\n",
      "learning_rate is  0.001\n",
      "The 6250 step train_total_loss is 0.15688525 val_total_loss is 0.18846023\n",
      "learning_rate is  0.001\n",
      "The 6300 step train_total_loss is 0.41234335 val_total_loss is 0.62953544\n",
      "learning_rate is  0.001\n",
      "The 6350 step train_total_loss is 0.25950682 val_total_loss is 0.23868416\n",
      "learning_rate is  0.001\n",
      "The 6400 step train_total_loss is 0.06553721 val_total_loss is 0.25148296\n",
      "learning_rate is  0.001\n",
      "The 6450 step train_total_loss is 0.6314745 val_total_loss is 0.5071959\n",
      "learning_rate is  0.001\n",
      "The 6500 step train_total_loss is 0.49215096 val_total_loss is 1.553543\n",
      "learning_rate is  0.001\n",
      "The 6550 step train_total_loss is 0.68435544 val_total_loss is 0.2720124\n",
      "learning_rate is  0.001\n",
      "The 6600 step train_total_loss is 0.20839864 val_total_loss is 0.18121347\n",
      "learning_rate is  0.001\n",
      "The 6650 step train_total_loss is 0.770123 val_total_loss is 0.17857854\n",
      "learning_rate is  0.001\n",
      "The 6700 step train_total_loss is 0.14113018 val_total_loss is 0.061741743\n",
      "learning_rate is  0.001\n",
      "The 6750 step train_total_loss is 0.14842334 val_total_loss is 0.3180657\n",
      "learning_rate is  0.001\n",
      "The 6800 step train_total_loss is 0.31386244 val_total_loss is 0.321928\n",
      "learning_rate is  0.001\n",
      "The 6850 step train_total_loss is 0.4423538 val_total_loss is 0.56104875\n",
      "learning_rate is  0.001\n",
      "The 6900 step train_total_loss is 0.67833185 val_total_loss is 0.7156798\n",
      "learning_rate is  0.001\n",
      "The 6950 step train_total_loss is 0.19026536 val_total_loss is 0.28333703\n",
      "learning_rate is  0.001\n",
      "The 7000 step train_total_loss is 0.17669918 val_total_loss is 0.34494624\n",
      "learning_rate is  0.001\n",
      "The 7050 step train_total_loss is 0.4871874 val_total_loss is 0.14415999\n",
      "learning_rate is  0.001\n",
      "The 7100 step train_total_loss is 0.13939947 val_total_loss is 0.32882652\n",
      "learning_rate is  0.001\n",
      "The 7150 step train_total_loss is 0.37920395 val_total_loss is 0.2369212\n",
      "learning_rate is  0.001\n",
      "The 7200 step train_total_loss is 0.3311193 val_total_loss is 0.31940043\n",
      "learning_rate is  0.001\n",
      "The 7250 step train_total_loss is 0.35874912 val_total_loss is 0.7212608\n",
      "learning_rate is  0.001\n",
      "The 7300 step train_total_loss is 0.93646103 val_total_loss is 0.29449466\n",
      "learning_rate is  0.001\n",
      "The 7350 step train_total_loss is 0.717605 val_total_loss is 0.65606105\n",
      "learning_rate is  0.001\n",
      "The 7400 step train_total_loss is 0.75680804 val_total_loss is 0.088878326\n",
      "learning_rate is  0.001\n",
      "The 7450 step train_total_loss is 0.33851463 val_total_loss is 0.52864337\n",
      "learning_rate is  0.001\n",
      "The 7500 step train_total_loss is 0.07577065 val_total_loss is 0.6993754\n",
      "learning_rate is  0.001\n",
      "The 7550 step train_total_loss is 0.15445653 val_total_loss is 0.13727307\n",
      "learning_rate is  0.001\n",
      "The 7600 step train_total_loss is 0.4513913 val_total_loss is 0.31143504\n",
      "learning_rate is  0.001\n",
      "The 7650 step train_total_loss is 1.5884402 val_total_loss is 0.4321921\n",
      "learning_rate is  0.001\n",
      "The 7700 step train_total_loss is 0.43436664 val_total_loss is 0.49768946\n",
      "learning_rate is  0.001\n",
      "The 7750 step train_total_loss is 0.13015231 val_total_loss is 0.43164977\n",
      "learning_rate is  0.001\n",
      "The 7800 step train_total_loss is 0.35028556 val_total_loss is 1.0043093\n",
      "learning_rate is  0.001\n",
      "The 7850 step train_total_loss is 0.35188004 val_total_loss is 0.32366166\n",
      "learning_rate is  0.001\n",
      "The 7900 step train_total_loss is 0.110300496 val_total_loss is 0.059264325\n",
      "learning_rate is  0.001\n",
      "The 7950 step train_total_loss is 0.35448584 val_total_loss is 0.5475146\n",
      "learning_rate is  0.001\n",
      "The 8000 step train_total_loss is 0.420285 val_total_loss is 0.3123916\n",
      "learning_rate is  0.001\n",
      "The 8050 step train_total_loss is 0.17107977 val_total_loss is 0.2517503\n",
      "learning_rate is  0.001\n",
      "The 8100 step train_total_loss is 0.678926 val_total_loss is 1.1362528\n",
      "learning_rate is  0.001\n",
      "The 8150 step train_total_loss is 0.98569804 val_total_loss is 0.43182316\n",
      "learning_rate is  0.001\n",
      "The 8200 step train_total_loss is 0.69286007 val_total_loss is 0.28015608\n",
      "learning_rate is  0.001\n",
      "The 8250 step train_total_loss is 0.5407579 val_total_loss is 0.5497932\n",
      "learning_rate is  0.001\n",
      "The 8300 step train_total_loss is 0.21603294 val_total_loss is 0.8407686\n",
      "learning_rate is  0.001\n",
      "The 8350 step train_total_loss is 0.097758695 val_total_loss is 0.71658385\n",
      "learning_rate is  0.001\n",
      "The 8400 step train_total_loss is 0.12661134 val_total_loss is 0.43734807\n",
      "learning_rate is  0.001\n",
      "The 8450 step train_total_loss is 0.38853562 val_total_loss is 0.289133\n",
      "learning_rate is  0.001\n",
      "The 8500 step train_total_loss is 1.1085291 val_total_loss is 0.46329835\n",
      "learning_rate is  0.001\n",
      "The 8550 step train_total_loss is 1.6885502 val_total_loss is 0.51901966\n",
      "learning_rate is  0.001\n",
      "The 8600 step train_total_loss is 0.48446792 val_total_loss is 0.09941912\n",
      "learning_rate is  0.001\n",
      "The 8650 step train_total_loss is 0.4462576 val_total_loss is 0.24789381\n",
      "learning_rate is  0.001\n",
      "The 8700 step train_total_loss is 0.116753675 val_total_loss is 0.52914995\n",
      "learning_rate is  0.001\n",
      "The 8750 step train_total_loss is 1.3913777 val_total_loss is 0.35023153\n",
      "learning_rate is  0.001\n",
      "The 8800 step train_total_loss is 1.1723936 val_total_loss is 0.22375093\n",
      "learning_rate is  0.001\n",
      "The 8850 step train_total_loss is 0.35911354 val_total_loss is 0.28018865\n",
      "learning_rate is  0.001\n",
      "The 8900 step train_total_loss is 0.60135937 val_total_loss is 0.91136146\n",
      "learning_rate is  0.001\n",
      "The 8950 step train_total_loss is 0.40651518 val_total_loss is 0.040216044\n",
      "learning_rate is  0.001\n",
      "The 9000 step train_total_loss is 0.3156754 val_total_loss is 0.33111954\n",
      "learning_rate is  0.001\n",
      "The 9050 step train_total_loss is 1.2338171 val_total_loss is 0.7201336\n",
      "learning_rate is  0.001\n",
      "The 9100 step train_total_loss is 0.83848774 val_total_loss is 0.4650836\n",
      "learning_rate is  0.001\n",
      "The 9150 step train_total_loss is 0.42937264 val_total_loss is 0.8782946\n",
      "learning_rate is  0.001\n",
      "The 9200 step train_total_loss is 0.13359734 val_total_loss is 0.20254894\n",
      "learning_rate is  0.001\n",
      "The 9250 step train_total_loss is 0.45462677 val_total_loss is 0.8161393\n",
      "learning_rate is  0.001\n",
      "The 9300 step train_total_loss is 0.66013634 val_total_loss is 0.13734895\n",
      "learning_rate is  0.001\n",
      "The 9350 step train_total_loss is 0.12710847 val_total_loss is 0.3340223\n",
      "learning_rate is  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 9400 step train_total_loss is 0.5313782 val_total_loss is 0.042587824\n",
      "learning_rate is  0.001\n",
      "The 9450 step train_total_loss is 0.3839687 val_total_loss is 0.97541434\n",
      "learning_rate is  0.001\n",
      "The 9500 step train_total_loss is 0.9440693 val_total_loss is 0.3669292\n",
      "learning_rate is  0.001\n",
      "The 9550 step train_total_loss is 0.47466373 val_total_loss is 0.3298045\n",
      "learning_rate is  0.001\n",
      "The 9600 step train_total_loss is 0.14064941 val_total_loss is 0.26822844\n",
      "learning_rate is  0.001\n",
      "The 9650 step train_total_loss is 0.2650977 val_total_loss is 0.5355016\n",
      "learning_rate is  0.001\n",
      "The 9700 step train_total_loss is 0.24714887 val_total_loss is 0.60991895\n",
      "learning_rate is  0.001\n",
      "The 9750 step train_total_loss is 0.67007375 val_total_loss is 0.28382748\n",
      "learning_rate is  0.001\n",
      "The 9800 step train_total_loss is 0.45666745 val_total_loss is 0.52068746\n",
      "learning_rate is  0.001\n",
      "The 9850 step train_total_loss is 0.4681353 val_total_loss is 0.1474392\n",
      "learning_rate is  0.001\n",
      "The 9900 step train_total_loss is 0.71105856 val_total_loss is 0.5996126\n",
      "learning_rate is  0.001\n",
      "The 9950 step train_total_loss is 0.27670538 val_total_loss is 0.20608382\n",
      "learning_rate is  0.001\n",
      "The 10000 step train_total_loss is 0.27102536 val_total_loss is 0.6228316\n",
      "learning_rate is  0.001\n",
      "The 10050 step train_total_loss is 0.7385432 val_total_loss is 0.19052517\n",
      "learning_rate is  0.001\n",
      "The 10100 step train_total_loss is 0.33530855 val_total_loss is 0.1956841\n",
      "learning_rate is  0.001\n",
      "The 10150 step train_total_loss is 0.42791402 val_total_loss is 0.6126979\n",
      "learning_rate is  0.001\n",
      "The 10200 step train_total_loss is 0.38756344 val_total_loss is 0.2877649\n",
      "learning_rate is  0.001\n",
      "The 10250 step train_total_loss is 0.3196353 val_total_loss is 0.17876285\n",
      "learning_rate is  0.001\n",
      "The 10300 step train_total_loss is 0.12506774 val_total_loss is 0.24652039\n",
      "learning_rate is  0.001\n",
      "The 10350 step train_total_loss is 1.3179626 val_total_loss is 0.5920708\n",
      "learning_rate is  0.001\n",
      "The 10400 step train_total_loss is 0.8527822 val_total_loss is 0.29155496\n",
      "learning_rate is  0.001\n",
      "The 10450 step train_total_loss is 0.9598962 val_total_loss is 1.5394456\n",
      "learning_rate is  0.001\n",
      "The 10500 step train_total_loss is 0.419353 val_total_loss is 0.6253191\n",
      "learning_rate is  0.001\n",
      "The 10550 step train_total_loss is 0.08961198 val_total_loss is 0.13520655\n",
      "learning_rate is  0.001\n",
      "The 10600 step train_total_loss is 0.3480811 val_total_loss is 0.29341343\n",
      "learning_rate is  0.001\n",
      "The 10650 step train_total_loss is 0.27071556 val_total_loss is 0.5136685\n",
      "learning_rate is  0.001\n",
      "The 10700 step train_total_loss is 0.586425 val_total_loss is 0.98601186\n",
      "learning_rate is  0.001\n",
      "The 10750 step train_total_loss is 0.16865197 val_total_loss is 0.29245117\n",
      "learning_rate is  0.001\n",
      "The 10800 step train_total_loss is 0.12212572 val_total_loss is 0.10306028\n",
      "learning_rate is  0.001\n",
      "The 10850 step train_total_loss is 0.13661097 val_total_loss is 0.1367762\n",
      "learning_rate is  0.001\n",
      "The 10900 step train_total_loss is 0.6347348 val_total_loss is 0.45963076\n",
      "learning_rate is  0.001\n",
      "The 10950 step train_total_loss is 0.19467667 val_total_loss is 0.102052405\n",
      "learning_rate is  0.001\n",
      "The 11000 step train_total_loss is 0.8049164 val_total_loss is 0.35892975\n",
      "learning_rate is  0.001\n",
      "The 11050 step train_total_loss is 0.7171915 val_total_loss is 0.16747561\n",
      "learning_rate is  0.001\n",
      "The 11100 step train_total_loss is 0.5555695 val_total_loss is 0.088293396\n",
      "learning_rate is  0.001\n",
      "The 11150 step train_total_loss is 0.3904773 val_total_loss is 0.12714277\n",
      "learning_rate is  0.001\n",
      "The 11200 step train_total_loss is 0.68558687 val_total_loss is 0.2936378\n",
      "learning_rate is  0.001\n",
      "The 11250 step train_total_loss is 0.4730343 val_total_loss is 0.12524031\n",
      "learning_rate is  0.001\n",
      "The 11300 step train_total_loss is 0.24844742 val_total_loss is 0.3236619\n",
      "learning_rate is  0.001\n",
      "The 11350 step train_total_loss is 0.16495463 val_total_loss is 0.9511668\n",
      "learning_rate is  0.001\n",
      "The 11400 step train_total_loss is 0.5371993 val_total_loss is 0.23114944\n",
      "learning_rate is  0.001\n",
      "The 11450 step train_total_loss is 0.73753524 val_total_loss is 0.6952126\n",
      "learning_rate is  0.001\n",
      "The 11500 step train_total_loss is 0.31024998 val_total_loss is 0.20338109\n",
      "learning_rate is  0.001\n",
      "The 11550 step train_total_loss is 0.19327247 val_total_loss is 0.4364115\n",
      "learning_rate is  0.001\n",
      "The 11600 step train_total_loss is 0.940789 val_total_loss is 0.19916016\n",
      "learning_rate is  0.001\n",
      "The 11650 step train_total_loss is 0.14633279 val_total_loss is 0.26404613\n",
      "learning_rate is  0.001\n",
      "The 11700 step train_total_loss is 0.29426393 val_total_loss is 0.34318727\n",
      "learning_rate is  0.001\n",
      "The 11750 step train_total_loss is 0.5604554 val_total_loss is 0.19782804\n",
      "learning_rate is  0.001\n",
      "The 11800 step train_total_loss is 0.27613154 val_total_loss is 1.0307482\n",
      "learning_rate is  0.001\n",
      "The 11850 step train_total_loss is 1.0667932 val_total_loss is 0.18099922\n",
      "learning_rate is  0.001\n",
      "The 11900 step train_total_loss is 0.6279346 val_total_loss is 0.18419325\n",
      "learning_rate is  0.001\n",
      "The 11950 step train_total_loss is 0.45734107 val_total_loss is 1.0906407\n",
      "learning_rate is  0.001\n",
      "The 12000 step train_total_loss is 0.74265057 val_total_loss is 0.12742656\n",
      "learning_rate is  0.001\n",
      "The 12050 step train_total_loss is 0.15177411 val_total_loss is 0.16482712\n",
      "learning_rate is  0.001\n",
      "The 12100 step train_total_loss is 0.4736761 val_total_loss is 0.082406715\n",
      "learning_rate is  0.001\n",
      "The 12150 step train_total_loss is 0.2708835 val_total_loss is 0.53856903\n",
      "learning_rate is  0.001\n",
      "The 12200 step train_total_loss is 0.33113235 val_total_loss is 0.16714561\n",
      "learning_rate is  0.001\n",
      "The 12250 step train_total_loss is 0.22560446 val_total_loss is 0.1804688\n",
      "learning_rate is  0.001\n",
      "The 12300 step train_total_loss is 0.15368211 val_total_loss is 0.546266\n",
      "learning_rate is  0.001\n",
      "The 12350 step train_total_loss is 0.54139763 val_total_loss is 1.1025943\n",
      "learning_rate is  0.001\n",
      "The 12400 step train_total_loss is 0.15559332 val_total_loss is 0.49606997\n",
      "learning_rate is  0.001\n",
      "The 12450 step train_total_loss is 0.7094256 val_total_loss is 0.10992634\n",
      "learning_rate is  0.001\n",
      "The 12500 step train_total_loss is 0.30460173 val_total_loss is 1.0968412\n",
      "learning_rate is  0.001\n",
      "The 12550 step train_total_loss is 0.50225973 val_total_loss is 0.52707905\n",
      "learning_rate is  0.001\n",
      "The 12600 step train_total_loss is 0.17214474 val_total_loss is 0.35411096\n",
      "learning_rate is  0.001\n",
      "The 12650 step train_total_loss is 0.57292277 val_total_loss is 0.68309784\n",
      "learning_rate is  0.001\n",
      "The 12700 step train_total_loss is 0.57211536 val_total_loss is 0.16645637\n",
      "learning_rate is  0.001\n",
      "The 12750 step train_total_loss is 1.0599971 val_total_loss is 0.65751034\n",
      "learning_rate is  0.001\n",
      "The 12800 step train_total_loss is 0.68928313 val_total_loss is 0.5667556\n",
      "learning_rate is  0.001\n",
      "The 12850 step train_total_loss is 0.1423066 val_total_loss is 1.3411978\n",
      "learning_rate is  0.001\n",
      "The 12900 step train_total_loss is 0.15256348 val_total_loss is 1.1558014\n",
      "learning_rate is  0.001\n",
      "The 12950 step train_total_loss is 0.4214189 val_total_loss is 1.0510302\n",
      "learning_rate is  0.001\n",
      "The 13000 step train_total_loss is 0.42603034 val_total_loss is 0.28019264\n",
      "learning_rate is  0.001\n",
      "The 13050 step train_total_loss is 0.33985353 val_total_loss is 0.19994481\n",
      "learning_rate is  0.001\n",
      "The 13100 step train_total_loss is 0.2134365 val_total_loss is 0.56700855\n",
      "learning_rate is  0.001\n",
      "The 13150 step train_total_loss is 0.32934678 val_total_loss is 0.16030261\n",
      "learning_rate is  0.001\n",
      "The 13200 step train_total_loss is 0.25364745 val_total_loss is 0.9885109\n",
      "learning_rate is  0.001\n",
      "The 13250 step train_total_loss is 0.77624714 val_total_loss is 0.33746135\n",
      "learning_rate is  0.001\n",
      "The 13300 step train_total_loss is 0.10921581 val_total_loss is 0.95327604\n",
      "learning_rate is  0.001\n",
      "The 13350 step train_total_loss is 0.9784032 val_total_loss is 0.45843247\n",
      "learning_rate is  0.001\n",
      "The 13400 step train_total_loss is 0.37193656 val_total_loss is 0.16295335\n",
      "learning_rate is  0.001\n",
      "The 13450 step train_total_loss is 1.2067454 val_total_loss is 0.40420118\n",
      "learning_rate is  0.001\n",
      "The 13500 step train_total_loss is 0.22227979 val_total_loss is 0.21687374\n",
      "learning_rate is  0.001\n",
      "The 13550 step train_total_loss is 0.41421497 val_total_loss is 0.34297657\n",
      "learning_rate is  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 13600 step train_total_loss is 0.15934232 val_total_loss is 0.094983175\n",
      "learning_rate is  0.001\n",
      "The 13650 step train_total_loss is 0.15629518 val_total_loss is 0.3322241\n",
      "learning_rate is  0.001\n",
      "The 13700 step train_total_loss is 0.5431278 val_total_loss is 1.2893933\n",
      "learning_rate is  0.001\n",
      "The 13750 step train_total_loss is 0.18644491 val_total_loss is 0.06427208\n",
      "learning_rate is  0.001\n",
      "The 13800 step train_total_loss is 0.5305901 val_total_loss is 0.62942076\n",
      "learning_rate is  0.001\n",
      "The 13850 step train_total_loss is 0.9389546 val_total_loss is 0.4423432\n",
      "learning_rate is  0.001\n",
      "The 13900 step train_total_loss is 0.093795136 val_total_loss is 0.15144095\n",
      "learning_rate is  0.001\n",
      "The 13950 step train_total_loss is 0.5573693 val_total_loss is 0.2006041\n",
      "learning_rate is  0.001\n",
      "The 14000 step train_total_loss is 0.785847 val_total_loss is 0.16619295\n",
      "learning_rate is  0.001\n",
      "The 14050 step train_total_loss is 0.1358985 val_total_loss is 0.12109211\n",
      "learning_rate is  0.001\n",
      "The 14100 step train_total_loss is 1.1541237 val_total_loss is 0.40487003\n",
      "learning_rate is  0.001\n",
      "The 14150 step train_total_loss is 0.1970998 val_total_loss is 0.18744075\n",
      "learning_rate is  0.001\n",
      "The 14200 step train_total_loss is 0.93779653 val_total_loss is 0.16592416\n",
      "learning_rate is  0.001\n",
      "The 14250 step train_total_loss is 0.25551394 val_total_loss is 0.24099143\n",
      "learning_rate is  0.001\n",
      "The 14300 step train_total_loss is 0.16360077 val_total_loss is 0.1688213\n",
      "learning_rate is  0.001\n",
      "The 14350 step train_total_loss is 1.0301046 val_total_loss is 0.22251812\n",
      "learning_rate is  0.001\n",
      "The 14400 step train_total_loss is 0.21562181 val_total_loss is 0.20615661\n",
      "learning_rate is  0.001\n",
      "The 14450 step train_total_loss is 0.51019335 val_total_loss is 1.0396675\n",
      "learning_rate is  0.001\n",
      "The 14500 step train_total_loss is 0.3374797 val_total_loss is 0.36733267\n",
      "learning_rate is  0.001\n",
      "The 14550 step train_total_loss is 0.56098586 val_total_loss is 0.3774352\n",
      "learning_rate is  0.001\n",
      "The 14600 step train_total_loss is 0.96521086 val_total_loss is 0.23207287\n",
      "learning_rate is  0.001\n",
      "The 14650 step train_total_loss is 0.14036511 val_total_loss is 0.43243098\n",
      "learning_rate is  0.001\n",
      "The 14700 step train_total_loss is 0.13074495 val_total_loss is 0.2564225\n",
      "learning_rate is  0.001\n",
      "The 14750 step train_total_loss is 0.83604336 val_total_loss is 0.53491926\n",
      "learning_rate is  0.001\n",
      "The 14800 step train_total_loss is 1.6462134 val_total_loss is 0.044596367\n",
      "learning_rate is  0.001\n",
      "The 14850 step train_total_loss is 0.22303894 val_total_loss is 0.562605\n",
      "learning_rate is  0.001\n",
      "The 14900 step train_total_loss is 0.062233184 val_total_loss is 0.47411978\n",
      "learning_rate is  0.001\n",
      "The 14950 step train_total_loss is 0.38480031 val_total_loss is 0.12947212\n",
      "learning_rate is  0.001\n",
      "The 15000 step train_total_loss is 0.8316196 val_total_loss is 0.84543663\n",
      "learning_rate is  0.001\n",
      "The 15050 step train_total_loss is 0.8663204 val_total_loss is 0.6459314\n",
      "learning_rate is  0.001\n",
      "The 15100 step train_total_loss is 1.0393267 val_total_loss is 0.4729678\n",
      "learning_rate is  0.001\n",
      "The 15150 step train_total_loss is 0.59487045 val_total_loss is 0.94268584\n",
      "learning_rate is  0.001\n",
      "The 15200 step train_total_loss is 0.60815346 val_total_loss is 0.07292378\n",
      "learning_rate is  0.001\n",
      "The 15250 step train_total_loss is 0.2505274 val_total_loss is 0.35552952\n",
      "learning_rate is  0.001\n",
      "The 15300 step train_total_loss is 0.23699321 val_total_loss is 0.43730497\n",
      "learning_rate is  0.001\n",
      "The 15350 step train_total_loss is 0.3939682 val_total_loss is 0.18112938\n",
      "learning_rate is  0.001\n",
      "The 15400 step train_total_loss is 0.3013762 val_total_loss is 0.2049836\n",
      "learning_rate is  0.001\n",
      "The 15450 step train_total_loss is 0.29379764 val_total_loss is 0.70179015\n",
      "learning_rate is  0.001\n",
      "The 15500 step train_total_loss is 0.18250032 val_total_loss is 0.5155522\n",
      "learning_rate is  0.001\n",
      "The 15550 step train_total_loss is 0.46387255 val_total_loss is 0.24075547\n",
      "learning_rate is  0.001\n",
      "The 15600 step train_total_loss is 0.6885418 val_total_loss is 0.12603885\n",
      "learning_rate is  0.001\n",
      "The 15650 step train_total_loss is 0.09682195 val_total_loss is 0.8093731\n",
      "learning_rate is  0.001\n",
      "The 15700 step train_total_loss is 0.15919425 val_total_loss is 0.21357316\n",
      "learning_rate is  0.001\n",
      "The 15750 step train_total_loss is 0.13257107 val_total_loss is 0.5943952\n",
      "learning_rate is  0.001\n",
      "The 15800 step train_total_loss is 0.5224942 val_total_loss is 0.6283876\n",
      "learning_rate is  0.001\n",
      "The 15850 step train_total_loss is 0.129102 val_total_loss is 0.9617482\n",
      "learning_rate is  0.001\n",
      "The 15900 step train_total_loss is 0.1729137 val_total_loss is 0.2794583\n",
      "learning_rate is  0.001\n",
      "The 15950 step train_total_loss is 0.13852958 val_total_loss is 0.56520915\n",
      "learning_rate is  0.001\n",
      "The 16000 step train_total_loss is 0.2234896 val_total_loss is 0.14248179\n",
      "learning_rate is  0.001\n",
      "The 16050 step train_total_loss is 0.13339074 val_total_loss is 0.10158094\n",
      "learning_rate is  0.001\n",
      "The 16100 step train_total_loss is 0.17796984 val_total_loss is 0.2741212\n",
      "learning_rate is  0.001\n",
      "The 16150 step train_total_loss is 0.9111924 val_total_loss is 0.39099532\n",
      "learning_rate is  0.001\n",
      "The 16200 step train_total_loss is 0.7272661 val_total_loss is 0.15228932\n",
      "learning_rate is  0.001\n",
      "The 16250 step train_total_loss is 0.2801938 val_total_loss is 0.14763021\n",
      "learning_rate is  0.001\n",
      "The 16300 step train_total_loss is 0.18117365 val_total_loss is 0.52925485\n",
      "learning_rate is  0.001\n",
      "The 16350 step train_total_loss is 0.5872381 val_total_loss is 0.13982265\n",
      "learning_rate is  0.001\n",
      "The 16400 step train_total_loss is 0.24003212 val_total_loss is 0.19124946\n",
      "learning_rate is  0.001\n",
      "The 16450 step train_total_loss is 0.07520822 val_total_loss is 0.40635684\n",
      "learning_rate is  0.001\n",
      "The 16500 step train_total_loss is 0.93018955 val_total_loss is 0.4490862\n",
      "learning_rate is  0.001\n",
      "The 16550 step train_total_loss is 0.24769245 val_total_loss is 0.47282168\n",
      "learning_rate is  0.001\n",
      "The 16600 step train_total_loss is 0.58089006 val_total_loss is 0.37501827\n",
      "learning_rate is  0.001\n",
      "The 16650 step train_total_loss is 0.16856767 val_total_loss is 0.527149\n",
      "learning_rate is  0.001\n",
      "The 16700 step train_total_loss is 0.15394744 val_total_loss is 0.17400387\n",
      "learning_rate is  0.001\n",
      "The 16750 step train_total_loss is 0.12773842 val_total_loss is 0.75883806\n",
      "learning_rate is  0.001\n",
      "The 16800 step train_total_loss is 0.84432083 val_total_loss is 0.19067663\n",
      "learning_rate is  0.001\n",
      "The 16850 step train_total_loss is 0.99736774 val_total_loss is 0.6961528\n",
      "learning_rate is  0.001\n",
      "The 16900 step train_total_loss is 0.36548734 val_total_loss is 0.2755612\n",
      "learning_rate is  0.001\n",
      "The 16950 step train_total_loss is 0.21790713 val_total_loss is 0.6331997\n",
      "learning_rate is  0.001\n",
      "The 17000 step train_total_loss is 0.25729954 val_total_loss is 0.38347515\n",
      "learning_rate is  0.001\n",
      "The 17050 step train_total_loss is 0.12002474 val_total_loss is 0.31778547\n",
      "learning_rate is  0.001\n",
      "The 17100 step train_total_loss is 0.5340911 val_total_loss is 0.1203377\n",
      "learning_rate is  0.001\n",
      "The 17150 step train_total_loss is 0.40044913 val_total_loss is 0.033329587\n",
      "learning_rate is  0.001\n",
      "The 17200 step train_total_loss is 0.36035198 val_total_loss is 0.44594377\n",
      "learning_rate is  0.001\n",
      "The 17250 step train_total_loss is 0.28948247 val_total_loss is 0.19906238\n",
      "learning_rate is  0.001\n",
      "The 17300 step train_total_loss is 0.22541076 val_total_loss is 0.26976\n",
      "learning_rate is  0.001\n",
      "The 17350 step train_total_loss is 0.22518516 val_total_loss is 0.24167317\n",
      "learning_rate is  0.001\n",
      "The 17400 step train_total_loss is 0.11014562 val_total_loss is 0.09468263\n",
      "learning_rate is  0.001\n",
      "The 17450 step train_total_loss is 0.2604323 val_total_loss is 0.28949594\n",
      "learning_rate is  0.001\n",
      "The 17500 step train_total_loss is 0.5006843 val_total_loss is 0.09325446\n",
      "learning_rate is  0.001\n",
      "The 17550 step train_total_loss is 0.12109898 val_total_loss is 0.07997039\n",
      "learning_rate is  0.001\n",
      "The 17600 step train_total_loss is 0.15834518 val_total_loss is 0.10703722\n",
      "learning_rate is  0.001\n",
      "The 17650 step train_total_loss is 0.18628237 val_total_loss is 0.62892514\n",
      "learning_rate is  0.001\n",
      "The 17700 step train_total_loss is 0.8343145 val_total_loss is 0.90802443\n",
      "learning_rate is  0.001\n",
      "The 17750 step train_total_loss is 0.3125516 val_total_loss is 0.53608364\n",
      "learning_rate is  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 17800 step train_total_loss is 0.35269853 val_total_loss is 1.0010704\n",
      "learning_rate is  0.001\n",
      "The 17850 step train_total_loss is 0.27998966 val_total_loss is 0.7720747\n",
      "learning_rate is  0.001\n",
      "The 17900 step train_total_loss is 0.8983877 val_total_loss is 0.22174071\n",
      "learning_rate is  0.001\n",
      "The 17950 step train_total_loss is 0.3560379 val_total_loss is 0.16261162\n",
      "learning_rate is  0.001\n",
      "The 18000 step train_total_loss is 0.027501777 val_total_loss is 0.4693451\n",
      "learning_rate is  0.001\n",
      "The 18050 step train_total_loss is 0.41542006 val_total_loss is 0.16683175\n",
      "learning_rate is  0.001\n",
      "The 18100 step train_total_loss is 0.38567743 val_total_loss is 0.13804625\n",
      "learning_rate is  0.001\n",
      "The 18150 step train_total_loss is 0.33153722 val_total_loss is 0.6393812\n",
      "learning_rate is  0.001\n",
      "The 18200 step train_total_loss is 0.8749365 val_total_loss is 0.27410108\n",
      "learning_rate is  0.001\n",
      "The 18250 step train_total_loss is 0.91022134 val_total_loss is 0.09544652\n",
      "learning_rate is  0.001\n",
      "The 18300 step train_total_loss is 0.68698764 val_total_loss is 0.4994445\n",
      "learning_rate is  0.001\n",
      "The 18350 step train_total_loss is 0.37808737 val_total_loss is 0.2051073\n",
      "learning_rate is  0.001\n",
      "The 18400 step train_total_loss is 0.3103119 val_total_loss is 0.40001297\n",
      "learning_rate is  0.001\n",
      "The 18450 step train_total_loss is 0.19448373 val_total_loss is 0.6154268\n",
      "learning_rate is  0.001\n",
      "The 18500 step train_total_loss is 0.12505333 val_total_loss is 0.11403929\n",
      "learning_rate is  0.001\n",
      "The 18550 step train_total_loss is 0.46891922 val_total_loss is 0.37207872\n",
      "learning_rate is  0.001\n",
      "The 18600 step train_total_loss is 0.59076405 val_total_loss is 0.21188697\n",
      "learning_rate is  0.001\n",
      "The 18650 step train_total_loss is 0.15320581 val_total_loss is 0.15571311\n",
      "learning_rate is  0.001\n",
      "The 18700 step train_total_loss is 0.6482226 val_total_loss is 0.20828685\n",
      "learning_rate is  0.001\n",
      "The 18750 step train_total_loss is 1.2494185 val_total_loss is 0.44767725\n",
      "learning_rate is  0.001\n",
      "The 18800 step train_total_loss is 0.7764391 val_total_loss is 0.21105182\n",
      "learning_rate is  0.001\n",
      "The 18850 step train_total_loss is 0.63462204 val_total_loss is 0.30391723\n",
      "learning_rate is  0.001\n",
      "The 18900 step train_total_loss is 0.22190067 val_total_loss is 0.33306956\n",
      "learning_rate is  0.001\n",
      "The 18950 step train_total_loss is 0.4249248 val_total_loss is 0.14190818\n",
      "learning_rate is  0.001\n",
      "The 19000 step train_total_loss is 0.27733272 val_total_loss is 0.8474653\n",
      "learning_rate is  0.001\n",
      "The 19050 step train_total_loss is 0.17161807 val_total_loss is 0.2653508\n",
      "learning_rate is  0.001\n",
      "The 19100 step train_total_loss is 0.056241065 val_total_loss is 0.1819576\n",
      "learning_rate is  0.001\n",
      "The 19150 step train_total_loss is 0.080416024 val_total_loss is 0.9472058\n",
      "learning_rate is  0.001\n",
      "The 19200 step train_total_loss is 0.7498997 val_total_loss is 0.21934094\n",
      "learning_rate is  0.001\n",
      "The 19250 step train_total_loss is 0.15619779 val_total_loss is 0.060052544\n",
      "learning_rate is  0.001\n",
      "The 19300 step train_total_loss is 0.121640004 val_total_loss is 0.8912456\n",
      "learning_rate is  0.001\n",
      "The 19350 step train_total_loss is 0.8511585 val_total_loss is 0.24470416\n",
      "learning_rate is  0.001\n",
      "The 19400 step train_total_loss is 0.6714302 val_total_loss is 0.17738758\n",
      "learning_rate is  0.001\n",
      "The 19450 step train_total_loss is 0.48624164 val_total_loss is 0.32973665\n",
      "learning_rate is  0.001\n",
      "The 19500 step train_total_loss is 0.54984 val_total_loss is 0.24440192\n",
      "learning_rate is  0.001\n",
      "The 19550 step train_total_loss is 1.0297072 val_total_loss is 0.8287442\n",
      "learning_rate is  0.001\n",
      "The 19600 step train_total_loss is 0.1439956 val_total_loss is 0.20096442\n",
      "learning_rate is  0.001\n",
      "The 19650 step train_total_loss is 0.66967875 val_total_loss is 0.15852267\n",
      "learning_rate is  0.001\n",
      "The 19700 step train_total_loss is 0.17786315 val_total_loss is 0.60813034\n",
      "learning_rate is  0.001\n",
      "The 19750 step train_total_loss is 0.45018458 val_total_loss is 0.21117085\n",
      "learning_rate is  0.001\n",
      "The 19800 step train_total_loss is 0.35974735 val_total_loss is 0.19422325\n",
      "learning_rate is  0.001\n",
      "The 19850 step train_total_loss is 0.3780513 val_total_loss is 0.10825842\n",
      "learning_rate is  0.001\n",
      "The 19900 step train_total_loss is 1.0450556 val_total_loss is 0.08179384\n",
      "learning_rate is  0.001\n",
      "The 19950 step train_total_loss is 0.4399566 val_total_loss is 0.16375054\n",
      "learning_rate is  0.001\n",
      "The 20000 step train_total_loss is 0.055374216 val_total_loss is 0.22257975\n",
      "learning_rate is  0.001\n",
      "The 20050 step train_total_loss is 0.23441932 val_total_loss is 0.94942695\n",
      "learning_rate is  0.001\n",
      "The 20100 step train_total_loss is 0.14145328 val_total_loss is 0.25849503\n",
      "learning_rate is  0.001\n",
      "The 20150 step train_total_loss is 0.6575301 val_total_loss is 0.6714251\n",
      "learning_rate is  0.001\n",
      "The 20200 step train_total_loss is 0.09632612 val_total_loss is 0.3944009\n",
      "learning_rate is  0.001\n",
      "The 20250 step train_total_loss is 0.14330593 val_total_loss is 0.10071962\n",
      "learning_rate is  0.001\n",
      "The 20300 step train_total_loss is 0.44965735 val_total_loss is 0.12401325\n",
      "learning_rate is  0.001\n",
      "The 20350 step train_total_loss is 0.3436476 val_total_loss is 0.076599546\n",
      "learning_rate is  0.001\n",
      "The 20400 step train_total_loss is 0.32653815 val_total_loss is 0.47882414\n",
      "learning_rate is  0.001\n",
      "The 20450 step train_total_loss is 0.114386216 val_total_loss is 0.06363624\n",
      "learning_rate is  0.001\n",
      "The 20500 step train_total_loss is 0.26612237 val_total_loss is 0.17526704\n",
      "learning_rate is  0.001\n",
      "The 20550 step train_total_loss is 0.4435928 val_total_loss is 0.29622173\n",
      "learning_rate is  0.001\n",
      "The 20600 step train_total_loss is 0.19445458 val_total_loss is 0.40734813\n",
      "learning_rate is  0.001\n",
      "The 20650 step train_total_loss is 0.14962801 val_total_loss is 0.18555233\n",
      "learning_rate is  0.001\n",
      "The 20700 step train_total_loss is 0.18499275 val_total_loss is 0.15013471\n",
      "learning_rate is  0.001\n",
      "The 20750 step train_total_loss is 0.20460312 val_total_loss is 0.008474505\n",
      "learning_rate is  0.001\n",
      "The 20800 step train_total_loss is 0.030684117 val_total_loss is 0.2978484\n",
      "learning_rate is  0.001\n",
      "The 20850 step train_total_loss is 0.27481902 val_total_loss is 0.1337112\n",
      "learning_rate is  0.001\n",
      "The 20900 step train_total_loss is 0.081513844 val_total_loss is 0.2490699\n",
      "learning_rate is  0.001\n",
      "The 20950 step train_total_loss is 0.51989716 val_total_loss is 0.18578428\n",
      "learning_rate is  0.001\n",
      "The 21000 step train_total_loss is 0.32614735 val_total_loss is 0.16641428\n",
      "learning_rate is  0.001\n",
      "The 21050 step train_total_loss is 0.42248386 val_total_loss is 0.3247763\n",
      "learning_rate is  0.001\n",
      "The 21100 step train_total_loss is 0.45009953 val_total_loss is 0.39322764\n",
      "learning_rate is  0.001\n",
      "The 21150 step train_total_loss is 0.6007625 val_total_loss is 0.03522972\n",
      "learning_rate is  0.001\n",
      "The 21200 step train_total_loss is 0.30676392 val_total_loss is 0.12437985\n",
      "learning_rate is  0.001\n",
      "The 21250 step train_total_loss is 0.1415941 val_total_loss is 0.85434574\n",
      "learning_rate is  0.001\n",
      "The 21300 step train_total_loss is 0.20475742 val_total_loss is 0.25536883\n",
      "learning_rate is  0.001\n",
      "The 21350 step train_total_loss is 0.772122 val_total_loss is 0.15311295\n",
      "learning_rate is  0.001\n",
      "The 21400 step train_total_loss is 0.45707554 val_total_loss is 0.07100584\n",
      "learning_rate is  0.001\n",
      "The 21450 step train_total_loss is 0.21592335 val_total_loss is 0.37031528\n",
      "learning_rate is  0.001\n",
      "The 21500 step train_total_loss is 0.28175962 val_total_loss is 1.2677296\n",
      "learning_rate is  0.001\n",
      "The 21550 step train_total_loss is 0.48599 val_total_loss is 0.16083215\n",
      "learning_rate is  0.001\n",
      "The 21600 step train_total_loss is 0.13080785 val_total_loss is 0.57408166\n",
      "learning_rate is  0.001\n",
      "The 21650 step train_total_loss is 0.40565273 val_total_loss is 0.23641777\n",
      "learning_rate is  0.001\n",
      "The 21700 step train_total_loss is 0.5214893 val_total_loss is 0.28958294\n",
      "learning_rate is  0.001\n",
      "The 21750 step train_total_loss is 0.62451184 val_total_loss is 0.055822134\n",
      "learning_rate is  0.001\n",
      "The 21800 step train_total_loss is 0.2528784 val_total_loss is 0.23575571\n",
      "learning_rate is  0.001\n",
      "The 21850 step train_total_loss is 0.42719662 val_total_loss is 0.09609635\n",
      "learning_rate is  0.001\n",
      "The 21900 step train_total_loss is 0.17914706 val_total_loss is 0.53495\n",
      "learning_rate is  0.001\n",
      "The 21950 step train_total_loss is 0.65826035 val_total_loss is 0.18789455\n",
      "learning_rate is  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 22000 step train_total_loss is 0.12794423 val_total_loss is 0.048470076\n",
      "learning_rate is  0.001\n",
      "The 22050 step train_total_loss is 0.10955246 val_total_loss is 0.13686475\n",
      "learning_rate is  0.001\n",
      "The 22100 step train_total_loss is 0.2300353 val_total_loss is 0.072089754\n",
      "learning_rate is  0.001\n",
      "The 22150 step train_total_loss is 0.045761697 val_total_loss is 0.5292311\n",
      "learning_rate is  0.001\n",
      "The 22200 step train_total_loss is 0.13739507 val_total_loss is 0.18984838\n",
      "learning_rate is  0.001\n",
      "The 22250 step train_total_loss is 0.30189884 val_total_loss is 0.49797082\n",
      "learning_rate is  0.001\n",
      "The 22300 step train_total_loss is 0.32237354 val_total_loss is 0.41024148\n",
      "learning_rate is  0.001\n",
      "The 22350 step train_total_loss is 0.37028933 val_total_loss is 0.37945473\n",
      "learning_rate is  0.001\n",
      "The 22400 step train_total_loss is 0.90408015 val_total_loss is 0.16049193\n",
      "learning_rate is  0.001\n",
      "The 22450 step train_total_loss is 0.20764472 val_total_loss is 0.15882277\n",
      "learning_rate is  0.001\n",
      "The 22500 step train_total_loss is 0.36602688 val_total_loss is 0.4811061\n",
      "learning_rate is  0.001\n",
      "The 22550 step train_total_loss is 0.11932486 val_total_loss is 0.24232122\n",
      "learning_rate is  0.001\n",
      "The 22600 step train_total_loss is 0.11157845 val_total_loss is 0.35897118\n",
      "learning_rate is  0.001\n",
      "The 22650 step train_total_loss is 0.82114416 val_total_loss is 0.6454662\n",
      "learning_rate is  0.001\n",
      "The 22700 step train_total_loss is 0.56584287 val_total_loss is 0.1122636\n",
      "learning_rate is  0.001\n",
      "The 22750 step train_total_loss is 0.31753534 val_total_loss is 0.60470426\n",
      "learning_rate is  0.001\n",
      "The 22800 step train_total_loss is 0.40590885 val_total_loss is 0.47848186\n",
      "learning_rate is  0.001\n",
      "The 22850 step train_total_loss is 0.25289303 val_total_loss is 0.20300457\n",
      "learning_rate is  0.001\n",
      "The 22900 step train_total_loss is 0.3412763 val_total_loss is 0.25944355\n",
      "learning_rate is  0.001\n",
      "The 22950 step train_total_loss is 0.6450499 val_total_loss is 0.24891259\n",
      "learning_rate is  0.001\n",
      "The 23000 step train_total_loss is 0.14940795 val_total_loss is 0.33010906\n",
      "learning_rate is  0.001\n",
      "The 23050 step train_total_loss is 0.11093874 val_total_loss is 0.16414423\n",
      "learning_rate is  0.001\n",
      "The 23100 step train_total_loss is 0.10973084 val_total_loss is 0.44126463\n",
      "learning_rate is  0.001\n",
      "The 23150 step train_total_loss is 0.18811932 val_total_loss is 0.6387906\n",
      "learning_rate is  0.001\n",
      "The 23200 step train_total_loss is 0.28241414 val_total_loss is 0.20988813\n",
      "learning_rate is  0.001\n",
      "The 23250 step train_total_loss is 0.47400486 val_total_loss is 0.12774915\n",
      "learning_rate is  0.001\n",
      "The 23300 step train_total_loss is 0.42130363 val_total_loss is 0.119693406\n",
      "learning_rate is  0.001\n",
      "The 23350 step train_total_loss is 1.1578918 val_total_loss is 0.3810262\n",
      "learning_rate is  0.001\n",
      "The 23400 step train_total_loss is 0.15084065 val_total_loss is 0.13080765\n",
      "learning_rate is  0.001\n",
      "The 23450 step train_total_loss is 0.12827557 val_total_loss is 0.8461489\n",
      "learning_rate is  0.001\n",
      "The 23500 step train_total_loss is 0.31630906 val_total_loss is 0.118712954\n",
      "learning_rate is  0.001\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Thu Oct 25 09:54:12 2018\n",
    "\n",
    "@author: LongJun\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import config as cfg\n",
    "import os\n",
    "import pascal_voc as pascl\n",
    "import tensorflow.contrib.slim as slim\n",
    "import anchor_generate\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "from anchor_label import anchor_labels_process, labels_generate, anchor_labels_process\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import network\n",
    "import datetime\n",
    "from losslayer import RPN_loss\n",
    "from predict_loss import Predict_loss\n",
    "# Solver Class, used for training\n",
    "# net: the name of backbone net, only support VGG16, more backbone net will be supported in the feature\n",
    "# data/val_data： trian_data/vla_data is a list that consist of dict of ground_truth label\n",
    "# rpn_loss class : used for calculating rpn_loss\n",
    "# predict_loss: used for calculating predict_loss\n",
    "class Solver(object):   \n",
    "    def __init__(self, net ,data, val_data, rpn_loss, predict_loss): \n",
    "        self.net = net\n",
    "        self.data = data\n",
    "        self.val_data = val_data\n",
    "        self.max_iter = cfg.MAX_ITER\n",
    "        self.lr = cfg.LEARNING_RATE\n",
    "        self.rpn_loss = rpn_loss\n",
    "        self.predict_loss = predict_loss\n",
    "        self.lr_change_ITER = cfg.lr_change_ITER\n",
    "        self.summary_iter = cfg.SUMMARY_ITER\n",
    "        self.save_iter = cfg.SAVE_ITER\n",
    "        self.overlaps_max = cfg.overlaps_max\n",
    "        self.overlaps_min = cfg.overlaps_min\n",
    "        self._variables_to_fix = {}\n",
    "        self.Summary_output = os.path.join(cfg.Summary_output, datetime.datetime.now().strftime('%Y_%m_%d_%H_%M'))\n",
    "        if not os._exists(self.Summary_output):\n",
    "            os.mkdir(self.Summary_output)\n",
    "        self.train_summary_dir = os.path.join(self.Summary_output, 'train')\n",
    "        self.val_summary_dir = os.path.join(self.Summary_output, 'val')\n",
    "        self.model_output_dir = os.path.join(cfg.OUTPUT_DIR) \n",
    "        if not os.path.exists(self.model_output_dir):\n",
    "            os.mkdir(self.model_output_dir)\n",
    "        if not os.path.exists(self.train_summary_dir):\n",
    "            os.mkdir(self.train_summary_dir)\n",
    "        if not os.path.exists(self.val_summary_dir):\n",
    "            os.mkdir(self.val_summary_dir)\n",
    "        self.ckpt_filename = os.path.join(self.model_output_dir, 'output.model')\n",
    "        \n",
    " # training process       \n",
    "    def train_model(self):\n",
    "        lr = tf.Variable(self.lr[0],trainable=False)\n",
    "        self.optimizer = tf.train.MomentumOptimizer(lr, cfg.momentum)\n",
    "        #self.optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "        self.loss = self.rpn_loss.add_loss() + self.predict_loss.add_loss()     \n",
    "        train_op = self.optimizer.minimize(self.loss)\n",
    "        variables = tf.global_variables()\n",
    "        reader = pywrap_tensorflow.NewCheckpointReader(self.net.weight_file_path)\n",
    "        var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "        variables_to_restore = self.get_var_list(variables, var_to_shape_map)\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver(var_list=variables_to_restore)\n",
    "        merged = tf.summary.merge_all()\n",
    "        with tf.Session() as sess:\n",
    "            train_writer = tf.summary.FileWriter(self.train_summary_dir, sess.graph)\n",
    "            val_writer = tf.summary.FileWriter(self.val_summary_dir)\n",
    "            sess.run(init)\n",
    "            saver.restore(sess, self.net.weight_file_path)\n",
    "            self.fix_variables(sess, self.net.weight_file_path)\n",
    "            saver = tf.train.Saver(variables,max_to_keep = 10)\n",
    "            for step in range(self.max_iter+1):\n",
    "                if step == self.lr_change_ITER:\n",
    "                    lr = tf.assign(lr, self.lr[1])\n",
    "                train_data = self.data.get()\n",
    "                image_height = np.array(train_data['image'].shape[1])\n",
    "                image_width = np.array(train_data['image'].shape[2])\n",
    "                feed_dict = {self.net.image: train_data['image'], self.net.image_width: image_width,\\\n",
    "                             self.net.image_height: image_height, self.net.gt_boxes: train_data['box'],\\\n",
    "                             self.net.gt_cls: train_data['cls']}\n",
    "                if step % self.summary_iter == 0:\n",
    "                    total_loss, summary, learning_rate= sess.run([self.loss, merged, lr], feed_dict=feed_dict)\n",
    "                    train_writer.add_summary(summary, step)\n",
    "                    val_data = self.val_data.get()\n",
    "                    val_image_height = np.array(val_data['image'].shape[1])\n",
    "                    val_image_width = np.array(val_data['image'].shape[2])\n",
    "                    val_feed_dict = {self.net.image: val_data['image'], self.net.image_width: val_image_width,\\\n",
    "                                     self.net.image_height: val_image_height, self.net.gt_boxes: val_data['box'],\\\n",
    "                                     self.net.gt_cls: val_data['cls']}\n",
    "                    val_loss, val_summary = sess.run([self.loss, merged], feed_dict=val_feed_dict)\n",
    "                    val_writer.add_summary(val_summary, step)\n",
    "                    print ('The', step, 'step train_total_loss is', total_loss, 'val_total_loss is', val_loss)\n",
    "                    print ('learning_rate is ', learning_rate)\n",
    "                if step % self.save_iter == 0:\n",
    "                    saver.save(sess, self.ckpt_filename, global_step = step)\n",
    "                sess.run(train_op, feed_dict=feed_dict)\n",
    "                    \n",
    "               \n",
    "                \n",
    "                \n",
    "#get the variables to restore               \n",
    "    def get_var_list(self, global_variables, ckpt_variables):\n",
    "        variables_to_restore = []\n",
    "        for key in global_variables:\n",
    "            print (key.name)\n",
    "            if key.name == ('vgg_16/fc6/weights:0') or key.name == ('vgg_16/fc7/weights:0'):\n",
    "                self._variables_to_fix[key.name] = key\n",
    "                continue\n",
    "            \n",
    "            if key.name.split(':')[0] in ckpt_variables:\n",
    "                variables_to_restore.append(key) \n",
    "        return variables_to_restore\n",
    "    \n",
    "#because fc6 and fc7 layers of pretrained vgg16 model is convolution format, so we need convert them to fully-connected layers\n",
    "    def fix_variables(self, sess, pretrained_model):\n",
    "        print('Fix VGG16 layers..')\n",
    "        with tf.variable_scope('Fix_VGG16') as scope:\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                fc6_conv = tf.get_variable(\"fc6_conv\", [7, 7, 512, 4096], trainable=False)\n",
    "                fc7_conv = tf.get_variable(\"fc7_conv\", [1, 1, 4096, 4096], trainable=False)\n",
    "                restorer_fc = tf.train.Saver({'vgg_16' + \"/fc6/weights\": fc6_conv, \n",
    "                                              'vgg_16' + \"/fc7/weights\": fc7_conv})\n",
    "                restorer_fc.restore(sess, pretrained_model)\n",
    "        \n",
    "                sess.run(tf.assign(self._variables_to_fix['vgg_16' + '/fc6/weights:0'], tf.reshape(fc6_conv, \n",
    "                                    self._variables_to_fix['vgg_16' + '/fc6/weights:0'].get_shape())))\n",
    "                sess.run(tf.assign(self._variables_to_fix['vgg_16' + '/fc7/weights:0'], tf.reshape(fc7_conv, \n",
    "                                    self._variables_to_fix['vgg_16' + '/fc7/weights:0'].get_shape())))\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = cfg.GPU_ID\n",
    "    net = network.Net()\n",
    "    rpn_loss_obj = RPN_loss(net.rois_output['rois_bbx'], net.all_anchors, net.gt_boxes, \\\n",
    "                        net.rois_output['rois_cls'], net.labels, net.anchor_obj)\n",
    "    predict_loss = Predict_loss(net._predictions[\"cls_score\"], net._proposal_targets['labels'],\\\n",
    "                                net._predictions['bbox_pred'], net._proposal_targets['bbox_targets'],\\\n",
    "                                net._proposal_targets['bbox_inside_weights'], net._proposal_targets['bbox_outside_weights'])\n",
    "    \n",
    "    train_data = pascl.pascal_voc(cfg.train_imdb_name, 'train', fliped=True)\n",
    "    val_data = pascl.pascal_voc(cfg.test_imdb_name, 'test', fliped=False)\n",
    "    solver = Solver(net, train_data, val_data, rpn_loss_obj, predict_loss)\n",
    "    print ('start training')\n",
    "    start = datetime.datetime.now()\n",
    "    solver.train_model()\n",
    "    end = datetime.now()\n",
    "    latency = (end - start).total_seconds()  \n",
    "    print(\"The latency is   \"+str(latency)+\"秒\")  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
