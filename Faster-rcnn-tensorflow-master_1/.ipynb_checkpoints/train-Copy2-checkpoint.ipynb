{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/network.py:28: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/network.py:104: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tfplot/ops.py:114: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/network.py:192: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/network.py:64: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/network.py:247: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/network.py:266: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/losslayer.py:26: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Loading gt_labels from: annotation_cache/VOC2007_trainval/pascal_train_gt_labels.pkl\n",
      "Appending horizontally-flipped training examples ...\n",
      "Loading gt_labels from: annotation_cache/VOC2007_test/pascal_test_gt_labels.pkl\n",
      "start training\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/losslayer.py:99: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "reg\n",
      "Tensor(\"Sum:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/losslayer.py:120: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "vgg_16/conv1/conv1_1/weights:0\n",
      "vgg_16/conv1/conv1_1/biases:0\n",
      "vgg_16/conv1/conv1_2/weights:0\n",
      "vgg_16/conv1/conv1_2/biases:0\n",
      "vgg_16/conv2/conv2_1/weights:0\n",
      "vgg_16/conv2/conv2_1/biases:0\n",
      "vgg_16/conv2/conv2_2/weights:0\n",
      "vgg_16/conv2/conv2_2/biases:0\n",
      "vgg_16/conv3/conv3_1/weights:0\n",
      "vgg_16/conv3/conv3_1/biases:0\n",
      "vgg_16/conv3/conv3_2/weights:0\n",
      "vgg_16/conv3/conv3_2/biases:0\n",
      "vgg_16/conv3/conv3_3/weights:0\n",
      "vgg_16/conv3/conv3_3/biases:0\n",
      "vgg_16/conv4/conv4_1/weights:0\n",
      "vgg_16/conv4/conv4_1/biases:0\n",
      "vgg_16/conv4/conv4_2/weights:0\n",
      "vgg_16/conv4/conv4_2/biases:0\n",
      "vgg_16/conv4/conv4_3/weights:0\n",
      "vgg_16/conv4/conv4_3/biases:0\n",
      "vgg_16/conv5/conv5_1/weights:0\n",
      "vgg_16/conv5/conv5_1/biases:0\n",
      "vgg_16/conv5/conv5_2/weights:0\n",
      "vgg_16/conv5/conv5_2/biases:0\n",
      "vgg_16/conv5/conv5_3/weights:0\n",
      "vgg_16/conv5/conv5_3/biases:0\n",
      "rpn/conv6/weights:0\n",
      "rpn/conv6/biases:0\n",
      "rpn/conv7/weights:0\n",
      "rpn/conv7/biases:0\n",
      "rpn/conv8/weights:0\n",
      "rpn/conv8/biases:0\n",
      "vgg_16/fc6/weights:0\n",
      "vgg_16/fc6/biases:0\n",
      "vgg_16/fc7/weights:0\n",
      "vgg_16/fc7/biases:0\n",
      "vgg_16/region_deciton/cls_score/weights:0\n",
      "vgg_16/region_deciton/cls_score/biases:0\n",
      "vgg_16/region_deciton/bbox_pred/weights:0\n",
      "vgg_16/region_deciton/bbox_pred/biases:0\n",
      "Variable:0\n",
      "vgg_16/conv3/conv3_1/weights/Momentum:0\n",
      "vgg_16/conv3/conv3_1/biases/Momentum:0\n",
      "vgg_16/conv3/conv3_2/weights/Momentum:0\n",
      "vgg_16/conv3/conv3_2/biases/Momentum:0\n",
      "vgg_16/conv3/conv3_3/weights/Momentum:0\n",
      "vgg_16/conv3/conv3_3/biases/Momentum:0\n",
      "vgg_16/conv4/conv4_1/weights/Momentum:0\n",
      "vgg_16/conv4/conv4_1/biases/Momentum:0\n",
      "vgg_16/conv4/conv4_2/weights/Momentum:0\n",
      "vgg_16/conv4/conv4_2/biases/Momentum:0\n",
      "vgg_16/conv4/conv4_3/weights/Momentum:0\n",
      "vgg_16/conv4/conv4_3/biases/Momentum:0\n",
      "vgg_16/conv5/conv5_1/weights/Momentum:0\n",
      "vgg_16/conv5/conv5_1/biases/Momentum:0\n",
      "vgg_16/conv5/conv5_2/weights/Momentum:0\n",
      "vgg_16/conv5/conv5_2/biases/Momentum:0\n",
      "vgg_16/conv5/conv5_3/weights/Momentum:0\n",
      "vgg_16/conv5/conv5_3/biases/Momentum:0\n",
      "rpn/conv6/weights/Momentum:0\n",
      "rpn/conv6/biases/Momentum:0\n",
      "rpn/conv7/weights/Momentum:0\n",
      "rpn/conv7/biases/Momentum:0\n",
      "rpn/conv8/weights/Momentum:0\n",
      "rpn/conv8/biases/Momentum:0\n",
      "vgg_16/fc6/weights/Momentum:0\n",
      "vgg_16/fc6/biases/Momentum:0\n",
      "vgg_16/fc7/weights/Momentum:0\n",
      "vgg_16/fc7/biases/Momentum:0\n",
      "vgg_16/region_deciton/cls_score/weights/Momentum:0\n",
      "vgg_16/region_deciton/cls_score/biases/Momentum:0\n",
      "vgg_16/region_deciton/bbox_pred/weights/Momentum:0\n",
      "vgg_16/region_deciton/bbox_pred/biases/Momentum:0\n",
      "INFO:tensorflow:Restoring parameters from model_pretrained/vgg_16.ckpt\n",
      "Fix VGG16 layers..\n",
      "INFO:tensorflow:Restoring parameters from model_pretrained/vgg_16.ckpt\n",
      "The 0 step train_total_loss is 4.41828 val_total_loss is 3.8873568\n",
      "learning_rate is  0.001\n",
      "The 50 step train_total_loss is 0.5337094 val_total_loss is 0.1290658\n",
      "learning_rate is  0.001\n",
      "The 100 step train_total_loss is 0.11110025 val_total_loss is 0.12430682\n",
      "learning_rate is  0.001\n",
      "The 150 step train_total_loss is 0.0053755776 val_total_loss is 0.56465375\n",
      "learning_rate is  0.001\n",
      "The 200 step train_total_loss is 1.8434798 val_total_loss is 2.3173428\n",
      "learning_rate is  0.001\n",
      "The 250 step train_total_loss is 2.5030632 val_total_loss is 0.7045532\n",
      "learning_rate is  0.001\n",
      "The 300 step train_total_loss is 1.4980378 val_total_loss is 0.25457156\n",
      "learning_rate is  0.001\n",
      "The 350 step train_total_loss is 1.4535435 val_total_loss is 0.55841625\n",
      "learning_rate is  0.001\n",
      "The 400 step train_total_loss is 0.8313574 val_total_loss is 0.08231215\n",
      "learning_rate is  0.001\n",
      "The 450 step train_total_loss is 0.8328324 val_total_loss is 1.2803819\n",
      "learning_rate is  0.001\n",
      "The 500 step train_total_loss is 0.073170334 val_total_loss is 0.20306899\n",
      "learning_rate is  0.001\n",
      "The 550 step train_total_loss is 0.8263453 val_total_loss is 0.4189361\n",
      "learning_rate is  0.001\n",
      "The 600 step train_total_loss is 0.6637931 val_total_loss is 0.63824147\n",
      "learning_rate is  0.001\n",
      "The 650 step train_total_loss is 0.5700067 val_total_loss is 2.033746\n",
      "learning_rate is  0.001\n",
      "The 700 step train_total_loss is 1.4299955 val_total_loss is 0.5543766\n",
      "learning_rate is  0.001\n",
      "The 750 step train_total_loss is 0.5472795 val_total_loss is 0.4376878\n",
      "learning_rate is  0.001\n",
      "The 800 step train_total_loss is 0.7774287 val_total_loss is 0.52590334\n",
      "learning_rate is  0.001\n",
      "The 850 step train_total_loss is 0.4831185 val_total_loss is 0.58538944\n",
      "learning_rate is  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 900 step train_total_loss is 2.0383534 val_total_loss is 0.47022986\n",
      "learning_rate is  0.001\n",
      "The 950 step train_total_loss is 1.0776887 val_total_loss is 1.1536682\n",
      "learning_rate is  0.001\n",
      "The 1000 step train_total_loss is 0.08342953 val_total_loss is 0.23132598\n",
      "learning_rate is  0.001\n",
      "The 1050 step train_total_loss is 0.22027113 val_total_loss is 0.91630447\n",
      "learning_rate is  0.001\n",
      "The 1100 step train_total_loss is 0.33405852 val_total_loss is 0.14086036\n",
      "learning_rate is  0.001\n",
      "The 1150 step train_total_loss is 1.7373806 val_total_loss is 0.55849636\n",
      "learning_rate is  0.001\n",
      "The 1200 step train_total_loss is 0.6139545 val_total_loss is 0.560928\n",
      "learning_rate is  0.001\n",
      "The 1250 step train_total_loss is 0.9352032 val_total_loss is 0.40485883\n",
      "learning_rate is  0.001\n",
      "The 1300 step train_total_loss is 0.20935577 val_total_loss is 0.17264569\n",
      "learning_rate is  0.001\n",
      "The 1350 step train_total_loss is 0.53258395 val_total_loss is 0.41233888\n",
      "learning_rate is  0.001\n",
      "The 1400 step train_total_loss is 0.6461364 val_total_loss is 0.48966607\n",
      "learning_rate is  0.001\n",
      "The 1450 step train_total_loss is 0.1628875 val_total_loss is 0.33217824\n",
      "learning_rate is  0.001\n",
      "The 1500 step train_total_loss is 0.9695747 val_total_loss is 0.31706747\n",
      "learning_rate is  0.001\n",
      "The 1550 step train_total_loss is 0.5000927 val_total_loss is 0.5815847\n",
      "learning_rate is  0.001\n",
      "The 1600 step train_total_loss is 0.6361264 val_total_loss is 0.28990698\n",
      "learning_rate is  0.001\n",
      "The 1650 step train_total_loss is 0.2230207 val_total_loss is 0.47882706\n",
      "learning_rate is  0.001\n",
      "The 1700 step train_total_loss is 0.77226204 val_total_loss is 0.044087984\n",
      "learning_rate is  0.001\n",
      "The 1750 step train_total_loss is 0.31815583 val_total_loss is 0.2501505\n",
      "learning_rate is  0.001\n",
      "The 1800 step train_total_loss is 0.025371538 val_total_loss is 0.33597618\n",
      "learning_rate is  0.001\n",
      "The 1850 step train_total_loss is 0.25984842 val_total_loss is 0.43730164\n",
      "learning_rate is  0.001\n",
      "The 1900 step train_total_loss is 1.0952291 val_total_loss is 0.27135143\n",
      "learning_rate is  0.001\n",
      "The 1950 step train_total_loss is 0.5797496 val_total_loss is 0.8518301\n",
      "learning_rate is  0.001\n",
      "The 2000 step train_total_loss is 0.22453204 val_total_loss is 0.4193285\n",
      "learning_rate is  0.001\n",
      "The 2050 step train_total_loss is 0.45929918 val_total_loss is 0.2309261\n",
      "learning_rate is  0.001\n",
      "The 2100 step train_total_loss is 0.32247782 val_total_loss is 0.2986437\n",
      "learning_rate is  0.001\n",
      "The 2150 step train_total_loss is 0.17868315 val_total_loss is 1.0955414\n",
      "learning_rate is  0.001\n",
      "The 2200 step train_total_loss is 0.28044018 val_total_loss is 1.2613016\n",
      "learning_rate is  0.001\n",
      "The 2250 step train_total_loss is 0.2834631 val_total_loss is 0.4060187\n",
      "learning_rate is  0.001\n",
      "The 2300 step train_total_loss is 0.43291706 val_total_loss is 0.07580518\n",
      "learning_rate is  0.001\n",
      "The 2350 step train_total_loss is 1.2432066 val_total_loss is 0.8146291\n",
      "learning_rate is  0.001\n",
      "The 2400 step train_total_loss is 0.24985307 val_total_loss is 0.676999\n",
      "learning_rate is  0.001\n",
      "The 2450 step train_total_loss is 0.9021009 val_total_loss is 0.26388758\n",
      "learning_rate is  0.001\n",
      "The 2500 step train_total_loss is 0.907621 val_total_loss is 0.7905965\n",
      "learning_rate is  0.001\n",
      "The 2550 step train_total_loss is 0.7109672 val_total_loss is 0.49835452\n",
      "learning_rate is  0.001\n",
      "The 2600 step train_total_loss is 0.78667426 val_total_loss is 0.72334456\n",
      "learning_rate is  0.001\n",
      "The 2650 step train_total_loss is 0.20049009 val_total_loss is 0.2930589\n",
      "learning_rate is  0.001\n",
      "The 2700 step train_total_loss is 1.7727698 val_total_loss is 0.29836345\n",
      "learning_rate is  0.001\n",
      "The 2750 step train_total_loss is 0.4577554 val_total_loss is 0.15260568\n",
      "learning_rate is  0.001\n",
      "The 2800 step train_total_loss is 0.35847458 val_total_loss is 0.8812888\n",
      "learning_rate is  0.001\n",
      "The 2850 step train_total_loss is 0.17035426 val_total_loss is 0.058568537\n",
      "learning_rate is  0.001\n",
      "The 2900 step train_total_loss is 0.98326147 val_total_loss is 0.68180144\n",
      "learning_rate is  0.001\n",
      "The 2950 step train_total_loss is 1.0872201 val_total_loss is 0.46759686\n",
      "learning_rate is  0.001\n",
      "The 3000 step train_total_loss is 0.56114274 val_total_loss is 0.36234444\n",
      "learning_rate is  0.001\n",
      "The 3050 step train_total_loss is 0.24699241 val_total_loss is 0.21400043\n",
      "learning_rate is  0.001\n",
      "The 3100 step train_total_loss is 0.20370266 val_total_loss is 0.13628794\n",
      "learning_rate is  0.001\n",
      "The 3150 step train_total_loss is 0.27992046 val_total_loss is 0.49269694\n",
      "learning_rate is  0.001\n",
      "The 3200 step train_total_loss is 0.14526705 val_total_loss is 0.45125622\n",
      "learning_rate is  0.001\n",
      "The 3250 step train_total_loss is 0.72935414 val_total_loss is 0.59739095\n",
      "learning_rate is  0.001\n",
      "The 3300 step train_total_loss is 0.27552462 val_total_loss is 1.2377669\n",
      "learning_rate is  0.001\n",
      "The 3350 step train_total_loss is 1.771605 val_total_loss is 0.20896025\n",
      "learning_rate is  0.001\n",
      "The 3400 step train_total_loss is 0.41130495 val_total_loss is 0.11699118\n",
      "learning_rate is  0.001\n",
      "The 3450 step train_total_loss is 1.0962038 val_total_loss is 0.09796002\n",
      "learning_rate is  0.001\n",
      "The 3500 step train_total_loss is 0.37404937 val_total_loss is 0.18403749\n",
      "learning_rate is  0.001\n",
      "The 3550 step train_total_loss is 0.1222509 val_total_loss is 0.42064387\n",
      "learning_rate is  0.001\n",
      "The 3600 step train_total_loss is 0.10167974 val_total_loss is 0.18934816\n",
      "learning_rate is  0.001\n",
      "The 3650 step train_total_loss is 1.7547128 val_total_loss is 0.19023643\n",
      "learning_rate is  0.001\n",
      "The 3700 step train_total_loss is 0.46051365 val_total_loss is 0.31440374\n",
      "learning_rate is  0.001\n",
      "The 3750 step train_total_loss is 0.12550986 val_total_loss is 0.37416804\n",
      "learning_rate is  0.001\n",
      "The 3800 step train_total_loss is 0.6689534 val_total_loss is 0.9430139\n",
      "learning_rate is  0.001\n",
      "The 3850 step train_total_loss is 0.22793856 val_total_loss is 0.587542\n",
      "learning_rate is  0.001\n",
      "The 3900 step train_total_loss is 0.7476749 val_total_loss is 0.775429\n",
      "learning_rate is  0.001\n",
      "The 3950 step train_total_loss is 0.20874532 val_total_loss is 0.48258018\n",
      "learning_rate is  0.001\n",
      "The 4000 step train_total_loss is 0.69255054 val_total_loss is 0.32751328\n",
      "learning_rate is  0.001\n",
      "The 4050 step train_total_loss is 0.0544756 val_total_loss is 0.28007162\n",
      "learning_rate is  0.001\n",
      "The 4100 step train_total_loss is 0.40410984 val_total_loss is 0.19074343\n",
      "learning_rate is  0.001\n",
      "The 4150 step train_total_loss is 0.49960288 val_total_loss is 0.35004765\n",
      "learning_rate is  0.001\n",
      "The 4200 step train_total_loss is 0.7817716 val_total_loss is 0.24646057\n",
      "learning_rate is  0.001\n",
      "The 4250 step train_total_loss is 0.83809507 val_total_loss is 0.44865483\n",
      "learning_rate is  0.001\n",
      "The 4300 step train_total_loss is 0.18305607 val_total_loss is 0.23073544\n",
      "learning_rate is  0.001\n",
      "The 4350 step train_total_loss is 0.29183716 val_total_loss is 0.7714987\n",
      "learning_rate is  0.001\n",
      "The 4400 step train_total_loss is 0.1654653 val_total_loss is 0.34084743\n",
      "learning_rate is  0.001\n",
      "The 4450 step train_total_loss is 0.56331486 val_total_loss is 1.73354\n",
      "learning_rate is  0.001\n",
      "The 4500 step train_total_loss is 0.972052 val_total_loss is 0.45562312\n",
      "learning_rate is  0.001\n",
      "The 4550 step train_total_loss is 0.31865403 val_total_loss is 0.21221437\n",
      "learning_rate is  0.001\n",
      "The 4600 step train_total_loss is 0.4356267 val_total_loss is 0.27657598\n",
      "learning_rate is  0.001\n",
      "The 4650 step train_total_loss is 0.237172 val_total_loss is 0.222283\n",
      "learning_rate is  0.001\n",
      "The 4700 step train_total_loss is 1.7280742 val_total_loss is 0.6054971\n",
      "learning_rate is  0.001\n",
      "The 4750 step train_total_loss is 0.18220982 val_total_loss is 0.5659324\n",
      "learning_rate is  0.001\n",
      "The 4800 step train_total_loss is 2.651043 val_total_loss is 0.43682614\n",
      "learning_rate is  0.001\n",
      "The 4850 step train_total_loss is 0.39618486 val_total_loss is 0.8639815\n",
      "learning_rate is  0.001\n",
      "The 4900 step train_total_loss is 0.18890032 val_total_loss is 0.19472477\n",
      "learning_rate is  0.001\n",
      "The 4950 step train_total_loss is 0.18146166 val_total_loss is 0.38908237\n",
      "learning_rate is  0.001\n",
      "The 5000 step train_total_loss is 0.8157646 val_total_loss is 1.3726014\n",
      "learning_rate is  1e-04\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'datetime' has no attribute 'now'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-655f11e699ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0mlatency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The latency is   \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"秒\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'datetime' has no attribute 'now'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Thu Oct 25 09:54:12 2018\n",
    "\n",
    "@author: LongJun\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import config as cfg\n",
    "import os\n",
    "import pascal_voc as pascl\n",
    "import tensorflow.contrib.slim as slim\n",
    "import anchor_generate\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "from anchor_label import anchor_labels_process, labels_generate, anchor_labels_process\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import network\n",
    "import datetime\n",
    "from losslayer import RPN_loss\n",
    "from predict_loss import Predict_loss\n",
    "# Solver Class, used for training\n",
    "# net: the name of backbone net, only support VGG16, more backbone net will be supported in the feature\n",
    "# data/val_data： trian_data/vla_data is a list that consist of dict of ground_truth label\n",
    "# rpn_loss class : used for calculating rpn_loss\n",
    "# predict_loss: used for calculating predict_loss\n",
    "class Solver(object):   \n",
    "    def __init__(self, net ,data, val_data, rpn_loss, predict_loss): \n",
    "        self.net = net\n",
    "        self.data = data\n",
    "        self.val_data = val_data\n",
    "        self.max_iter = cfg.MAX_ITER\n",
    "        self.lr = cfg.LEARNING_RATE\n",
    "        self.rpn_loss = rpn_loss\n",
    "        self.predict_loss = predict_loss\n",
    "        self.lr_change_ITER = cfg.lr_change_ITER\n",
    "        self.summary_iter = cfg.SUMMARY_ITER\n",
    "        self.save_iter = cfg.SAVE_ITER\n",
    "        self.overlaps_max = cfg.overlaps_max\n",
    "        self.overlaps_min = cfg.overlaps_min\n",
    "        self._variables_to_fix = {}\n",
    "        self.Summary_output = os.path.join(cfg.Summary_output, datetime.datetime.now().strftime('%Y_%m_%d_%H_%M'))\n",
    "        if not os._exists(self.Summary_output):\n",
    "            os.mkdir(self.Summary_output)\n",
    "        self.train_summary_dir = os.path.join(self.Summary_output, 'train')\n",
    "        self.val_summary_dir = os.path.join(self.Summary_output, 'val')\n",
    "        self.model_output_dir = os.path.join(cfg.OUTPUT_DIR) \n",
    "        if not os.path.exists(self.model_output_dir):\n",
    "            os.mkdir(self.model_output_dir)\n",
    "        if not os.path.exists(self.train_summary_dir):\n",
    "            os.mkdir(self.train_summary_dir)\n",
    "        if not os.path.exists(self.val_summary_dir):\n",
    "            os.mkdir(self.val_summary_dir)\n",
    "        self.ckpt_filename = os.path.join(self.model_output_dir, 'output.model')\n",
    "        \n",
    " # training process       \n",
    "    def train_model(self):\n",
    "        lr = tf.Variable(self.lr[0],trainable=False)\n",
    "        self.optimizer = tf.train.MomentumOptimizer(lr, cfg.momentum)\n",
    "        #self.optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "        self.loss = self.rpn_loss.add_loss() + self.predict_loss.add_loss()     \n",
    "        train_op = self.optimizer.minimize(self.loss)\n",
    "        variables = tf.global_variables()\n",
    "        reader = pywrap_tensorflow.NewCheckpointReader(self.net.weight_file_path)\n",
    "        var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "        variables_to_restore = self.get_var_list(variables, var_to_shape_map)\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver(var_list=variables_to_restore)\n",
    "        merged = tf.summary.merge_all()\n",
    "        with tf.Session() as sess:\n",
    "            train_writer = tf.summary.FileWriter(self.train_summary_dir, sess.graph)\n",
    "            val_writer = tf.summary.FileWriter(self.val_summary_dir)\n",
    "            sess.run(init)\n",
    "            saver.restore(sess, self.net.weight_file_path)\n",
    "            self.fix_variables(sess, self.net.weight_file_path)\n",
    "            saver = tf.train.Saver(variables,max_to_keep = 10)\n",
    "            for step in range(self.max_iter+1):\n",
    "                if step == self.lr_change_ITER:\n",
    "                    lr = tf.assign(lr, self.lr[1])\n",
    "                train_data = self.data.get()\n",
    "                image_height = np.array(train_data['image'].shape[1])\n",
    "                image_width = np.array(train_data['image'].shape[2])\n",
    "                feed_dict = {self.net.image: train_data['image'], self.net.image_width: image_width,\\\n",
    "                             self.net.image_height: image_height, self.net.gt_boxes: train_data['box'],\\\n",
    "                             self.net.gt_cls: train_data['cls']}\n",
    "                if step % self.summary_iter == 0:\n",
    "                    total_loss, summary, learning_rate= sess.run([self.loss, merged, lr], feed_dict=feed_dict)\n",
    "                    train_writer.add_summary(summary, step)\n",
    "                    val_data = self.val_data.get()\n",
    "                    val_image_height = np.array(val_data['image'].shape[1])\n",
    "                    val_image_width = np.array(val_data['image'].shape[2])\n",
    "                    val_feed_dict = {self.net.image: val_data['image'], self.net.image_width: val_image_width,\\\n",
    "                                     self.net.image_height: val_image_height, self.net.gt_boxes: val_data['box'],\\\n",
    "                                     self.net.gt_cls: val_data['cls']}\n",
    "                    val_loss, val_summary = sess.run([self.loss, merged], feed_dict=val_feed_dict)\n",
    "                    val_writer.add_summary(val_summary, step)\n",
    "                    print ('The', step, 'step train_total_loss is', total_loss, 'val_total_loss is', val_loss)\n",
    "                    print ('learning_rate is ', learning_rate)\n",
    "                if step % self.save_iter == 0:\n",
    "                    saver.save(sess, self.ckpt_filename, global_step = step)\n",
    "                sess.run(train_op, feed_dict=feed_dict)\n",
    "                    \n",
    "               \n",
    "                \n",
    "                \n",
    "#get the variables to restore               \n",
    "    def get_var_list(self, global_variables, ckpt_variables):\n",
    "        variables_to_restore = []\n",
    "        for key in global_variables:\n",
    "            print (key.name)\n",
    "            if key.name == ('vgg_16/fc6/weights:0') or key.name == ('vgg_16/fc7/weights:0'):\n",
    "                self._variables_to_fix[key.name] = key\n",
    "                continue\n",
    "            \n",
    "            if key.name.split(':')[0] in ckpt_variables:\n",
    "                variables_to_restore.append(key) \n",
    "        return variables_to_restore\n",
    "    \n",
    "#because fc6 and fc7 layers of pretrained vgg16 model is convolution format, so we need convert them to fully-connected layers\n",
    "    def fix_variables(self, sess, pretrained_model):\n",
    "        print('Fix VGG16 layers..')\n",
    "        with tf.variable_scope('Fix_VGG16') as scope:\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                fc6_conv = tf.get_variable(\"fc6_conv\", [7, 7, 512, 4096], trainable=False)\n",
    "                fc7_conv = tf.get_variable(\"fc7_conv\", [1, 1, 4096, 4096], trainable=False)\n",
    "                restorer_fc = tf.train.Saver({'vgg_16' + \"/fc6/weights\": fc6_conv, \n",
    "                                              'vgg_16' + \"/fc7/weights\": fc7_conv})\n",
    "                restorer_fc.restore(sess, pretrained_model)\n",
    "        \n",
    "                sess.run(tf.assign(self._variables_to_fix['vgg_16' + '/fc6/weights:0'], tf.reshape(fc6_conv, \n",
    "                                    self._variables_to_fix['vgg_16' + '/fc6/weights:0'].get_shape())))\n",
    "                sess.run(tf.assign(self._variables_to_fix['vgg_16' + '/fc7/weights:0'], tf.reshape(fc7_conv, \n",
    "                                    self._variables_to_fix['vgg_16' + '/fc7/weights:0'].get_shape())))\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = cfg.GPU_ID\n",
    "    net = network.Net()\n",
    "    rpn_loss_obj = RPN_loss(net.rois_output['rois_bbx'], net.all_anchors, net.gt_boxes, \\\n",
    "                        net.rois_output['rois_cls'], net.labels, net.anchor_obj)\n",
    "    predict_loss = Predict_loss(net._predictions[\"cls_score\"], net._proposal_targets['labels'],\\\n",
    "                                net._predictions['bbox_pred'], net._proposal_targets['bbox_targets'],\\\n",
    "                                net._proposal_targets['bbox_inside_weights'], net._proposal_targets['bbox_outside_weights'])\n",
    "    \n",
    "    train_data = pascl.pascal_voc(cfg.train_imdb_name, 'train', fliped=True)\n",
    "    val_data = pascl.pascal_voc(cfg.test_imdb_name, 'test', fliped=False)\n",
    "    solver = Solver(net, train_data, val_data, rpn_loss_obj, predict_loss)\n",
    "    print ('start training')\n",
    "    start = datetime.datetime.now()\n",
    "    solver.train_model()\n",
    "    end = datetime.now()\n",
    "    latency = (end - start).total_seconds()  \n",
    "    print(\"The latency is   \"+str(latency)+\"秒\")  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
