{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/network.py:28: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/network.py:104: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tfplot/ops.py:114: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/network.py:192: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/network.py:64: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/network.py:247: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/network.py:266: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/losslayer.py:26: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Loading gt_labels from: annotation_cache/VOC2007_trainval/pascal_train_gt_labels.pkl\n",
      "Appending horizontally-flipped training examples ...\n",
      "Loading gt_labels from: annotation_cache/VOC2007_test/pascal_test_gt_labels.pkl\n",
      "start training\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/losslayer.py:99: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "reg\n",
      "Tensor(\"Sum:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:From /root/code/Faster-rcnn-tensorflow-master_1/losslayer.py:120: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "vgg_16/conv1/conv1_1/weights:0\n",
      "vgg_16/conv1/conv1_1/biases:0\n",
      "vgg_16/conv1/conv1_2/weights:0\n",
      "vgg_16/conv1/conv1_2/biases:0\n",
      "vgg_16/conv2/conv2_1/weights:0\n",
      "vgg_16/conv2/conv2_1/biases:0\n",
      "vgg_16/conv2/conv2_2/weights:0\n",
      "vgg_16/conv2/conv2_2/biases:0\n",
      "vgg_16/conv3/conv3_1/weights:0\n",
      "vgg_16/conv3/conv3_1/biases:0\n",
      "vgg_16/conv3/conv3_2/weights:0\n",
      "vgg_16/conv3/conv3_2/biases:0\n",
      "vgg_16/conv3/conv3_3/weights:0\n",
      "vgg_16/conv3/conv3_3/biases:0\n",
      "vgg_16/conv4/conv4_1/weights:0\n",
      "vgg_16/conv4/conv4_1/biases:0\n",
      "vgg_16/conv4/conv4_2/weights:0\n",
      "vgg_16/conv4/conv4_2/biases:0\n",
      "vgg_16/conv4/conv4_3/weights:0\n",
      "vgg_16/conv4/conv4_3/biases:0\n",
      "vgg_16/conv5/conv5_1/weights:0\n",
      "vgg_16/conv5/conv5_1/biases:0\n",
      "vgg_16/conv5/conv5_2/weights:0\n",
      "vgg_16/conv5/conv5_2/biases:0\n",
      "vgg_16/conv5/conv5_3/weights:0\n",
      "vgg_16/conv5/conv5_3/biases:0\n",
      "rpn/conv6/weights:0\n",
      "rpn/conv6/biases:0\n",
      "rpn/conv7/weights:0\n",
      "rpn/conv7/biases:0\n",
      "rpn/conv8/weights:0\n",
      "rpn/conv8/biases:0\n",
      "vgg_16/fc6/weights:0\n",
      "vgg_16/fc6/biases:0\n",
      "vgg_16/fc7/weights:0\n",
      "vgg_16/fc7/biases:0\n",
      "vgg_16/region_deciton/cls_score/weights:0\n",
      "vgg_16/region_deciton/cls_score/biases:0\n",
      "vgg_16/region_deciton/bbox_pred/weights:0\n",
      "vgg_16/region_deciton/bbox_pred/biases:0\n",
      "Variable:0\n",
      "vgg_16/conv3/conv3_1/weights/Momentum:0\n",
      "vgg_16/conv3/conv3_1/biases/Momentum:0\n",
      "vgg_16/conv3/conv3_2/weights/Momentum:0\n",
      "vgg_16/conv3/conv3_2/biases/Momentum:0\n",
      "vgg_16/conv3/conv3_3/weights/Momentum:0\n",
      "vgg_16/conv3/conv3_3/biases/Momentum:0\n",
      "vgg_16/conv4/conv4_1/weights/Momentum:0\n",
      "vgg_16/conv4/conv4_1/biases/Momentum:0\n",
      "vgg_16/conv4/conv4_2/weights/Momentum:0\n",
      "vgg_16/conv4/conv4_2/biases/Momentum:0\n",
      "vgg_16/conv4/conv4_3/weights/Momentum:0\n",
      "vgg_16/conv4/conv4_3/biases/Momentum:0\n",
      "vgg_16/conv5/conv5_1/weights/Momentum:0\n",
      "vgg_16/conv5/conv5_1/biases/Momentum:0\n",
      "vgg_16/conv5/conv5_2/weights/Momentum:0\n",
      "vgg_16/conv5/conv5_2/biases/Momentum:0\n",
      "vgg_16/conv5/conv5_3/weights/Momentum:0\n",
      "vgg_16/conv5/conv5_3/biases/Momentum:0\n",
      "rpn/conv6/weights/Momentum:0\n",
      "rpn/conv6/biases/Momentum:0\n",
      "rpn/conv7/weights/Momentum:0\n",
      "rpn/conv7/biases/Momentum:0\n",
      "rpn/conv8/weights/Momentum:0\n",
      "rpn/conv8/biases/Momentum:0\n",
      "vgg_16/fc6/weights/Momentum:0\n",
      "vgg_16/fc6/biases/Momentum:0\n",
      "vgg_16/fc7/weights/Momentum:0\n",
      "vgg_16/fc7/biases/Momentum:0\n",
      "vgg_16/region_deciton/cls_score/weights/Momentum:0\n",
      "vgg_16/region_deciton/cls_score/biases/Momentum:0\n",
      "vgg_16/region_deciton/bbox_pred/weights/Momentum:0\n",
      "vgg_16/region_deciton/bbox_pred/biases/Momentum:0\n",
      "INFO:tensorflow:Restoring parameters from model_pretrained/vgg_16.ckpt\n",
      "Fix VGG16 layers..\n",
      "INFO:tensorflow:Restoring parameters from model_pretrained/vgg_16.ckpt\n",
      "The 0 step train_total_loss is 3.631862 val_total_loss is 4.053889\n",
      "learning_rate is  0.001\n",
      "The 50 step train_total_loss is 0.17153713 val_total_loss is 0.08317742\n",
      "learning_rate is  0.001\n",
      "The 100 step train_total_loss is 0.27562624 val_total_loss is 0.1443598\n",
      "learning_rate is  0.001\n",
      "The 150 step train_total_loss is 0.6590849 val_total_loss is 0.1866286\n",
      "learning_rate is  0.001\n",
      "The 200 step train_total_loss is 0.13745485 val_total_loss is 0.19173521\n",
      "learning_rate is  0.001\n",
      "The 250 step train_total_loss is 0.114039585 val_total_loss is 0.047855675\n",
      "learning_rate is  0.001\n",
      "The 300 step train_total_loss is 0.14202388 val_total_loss is 0.16886437\n",
      "learning_rate is  0.001\n",
      "The 350 step train_total_loss is 0.18689148 val_total_loss is 0.037086207\n",
      "learning_rate is  0.001\n",
      "The 400 step train_total_loss is 0.06264853 val_total_loss is 0.042725835\n",
      "learning_rate is  0.001\n",
      "The 450 step train_total_loss is 0.013776411 val_total_loss is 0.24970561\n",
      "learning_rate is  0.001\n",
      "The 500 step train_total_loss is 0.10790678 val_total_loss is 0.03621875\n",
      "learning_rate is  0.001\n",
      "The 550 step train_total_loss is 0.18713246 val_total_loss is 0.095591426\n",
      "learning_rate is  0.001\n",
      "The 600 step train_total_loss is 0.60151833 val_total_loss is 0.29807523\n",
      "learning_rate is  0.001\n",
      "The 650 step train_total_loss is 0.24095052 val_total_loss is 2.3138332\n",
      "learning_rate is  0.001\n",
      "The 700 step train_total_loss is 0.34557524 val_total_loss is 0.14660761\n",
      "learning_rate is  0.001\n",
      "The 750 step train_total_loss is 0.2540629 val_total_loss is 0.4014924\n",
      "learning_rate is  0.001\n",
      "The 800 step train_total_loss is 0.14234994 val_total_loss is 0.24582699\n",
      "learning_rate is  0.001\n",
      "The 850 step train_total_loss is 0.09415832 val_total_loss is 0.14816107\n",
      "learning_rate is  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 900 step train_total_loss is 0.31148824 val_total_loss is 0.65272886\n",
      "learning_rate is  0.001\n",
      "The 950 step train_total_loss is 0.6227419 val_total_loss is 0.75160456\n",
      "learning_rate is  0.001\n",
      "The 1000 step train_total_loss is 0.11378308 val_total_loss is 0.48804605\n",
      "learning_rate is  0.001\n",
      "The 1050 step train_total_loss is 0.30758953 val_total_loss is 0.58776915\n",
      "learning_rate is  0.001\n",
      "The 1100 step train_total_loss is 0.60599315 val_total_loss is 0.3613932\n",
      "learning_rate is  0.001\n",
      "The 1150 step train_total_loss is 0.32620585 val_total_loss is 0.22705823\n",
      "learning_rate is  0.001\n",
      "The 1200 step train_total_loss is 0.22468808 val_total_loss is 0.6858279\n",
      "learning_rate is  0.001\n",
      "The 1250 step train_total_loss is 0.23801976 val_total_loss is 0.43291816\n",
      "learning_rate is  0.001\n",
      "The 1300 step train_total_loss is 0.3957008 val_total_loss is 0.21920916\n",
      "learning_rate is  0.001\n",
      "The 1350 step train_total_loss is 0.6980618 val_total_loss is 0.08128658\n",
      "learning_rate is  0.001\n",
      "The 1400 step train_total_loss is 0.4969516 val_total_loss is 0.35521218\n",
      "learning_rate is  0.001\n",
      "The 1450 step train_total_loss is 0.16202906 val_total_loss is 0.4883683\n",
      "learning_rate is  0.001\n",
      "The 1500 step train_total_loss is 1.1808252 val_total_loss is 0.24891439\n",
      "learning_rate is  0.001\n",
      "The 1550 step train_total_loss is 0.21375464 val_total_loss is 0.2600665\n",
      "learning_rate is  0.001\n",
      "The 1600 step train_total_loss is 1.2489219 val_total_loss is 0.3034443\n",
      "learning_rate is  0.001\n",
      "The 1650 step train_total_loss is 0.67111737 val_total_loss is 0.47998026\n",
      "learning_rate is  0.001\n",
      "The 1700 step train_total_loss is 0.26842675 val_total_loss is 0.08826201\n",
      "learning_rate is  0.001\n",
      "The 1750 step train_total_loss is 0.09212741 val_total_loss is 0.07395805\n",
      "learning_rate is  0.001\n",
      "The 1800 step train_total_loss is 0.25483912 val_total_loss is 0.29424506\n",
      "learning_rate is  0.001\n",
      "The 1850 step train_total_loss is 0.24700592 val_total_loss is 0.12626915\n",
      "learning_rate is  0.001\n",
      "The 1900 step train_total_loss is 0.08516641 val_total_loss is 0.040987838\n",
      "learning_rate is  0.001\n",
      "The 1950 step train_total_loss is 0.71536636 val_total_loss is 0.69123787\n",
      "learning_rate is  0.001\n",
      "The 2000 step train_total_loss is 1.024015 val_total_loss is 0.09484622\n",
      "learning_rate is  0.001\n",
      "The 2050 step train_total_loss is 0.98972887 val_total_loss is 0.24702625\n",
      "learning_rate is  0.001\n",
      "The 2100 step train_total_loss is 1.0295854 val_total_loss is 0.39781654\n",
      "learning_rate is  0.001\n",
      "The 2150 step train_total_loss is 0.88259375 val_total_loss is 2.1322367\n",
      "learning_rate is  0.001\n",
      "The 2200 step train_total_loss is 0.15796538 val_total_loss is 0.04886683\n",
      "learning_rate is  0.001\n",
      "The 2250 step train_total_loss is 0.36630505 val_total_loss is 0.053141598\n",
      "learning_rate is  0.001\n",
      "The 2300 step train_total_loss is 0.7913381 val_total_loss is 0.057758786\n",
      "learning_rate is  0.001\n",
      "The 2350 step train_total_loss is 0.37298852 val_total_loss is 0.6963554\n",
      "learning_rate is  0.001\n",
      "The 2400 step train_total_loss is 0.27824846 val_total_loss is 0.21422029\n",
      "learning_rate is  0.001\n",
      "The 2450 step train_total_loss is 0.22654486 val_total_loss is 0.4536735\n",
      "learning_rate is  0.001\n",
      "The 2500 step train_total_loss is 0.42962763 val_total_loss is 0.83601683\n",
      "learning_rate is  0.001\n",
      "The 2550 step train_total_loss is 0.1396421 val_total_loss is 0.43631992\n",
      "learning_rate is  0.001\n",
      "The 2600 step train_total_loss is 0.05206965 val_total_loss is 0.3656709\n",
      "learning_rate is  0.001\n",
      "The 2650 step train_total_loss is 0.09614926 val_total_loss is 0.10740637\n",
      "learning_rate is  0.001\n",
      "The 2700 step train_total_loss is 0.21813464 val_total_loss is 0.12502092\n",
      "learning_rate is  0.001\n",
      "The 2750 step train_total_loss is 0.4752832 val_total_loss is 0.18679804\n",
      "learning_rate is  0.001\n",
      "The 2800 step train_total_loss is 0.09265526 val_total_loss is 1.0558852\n",
      "learning_rate is  0.001\n",
      "The 2850 step train_total_loss is 0.41084462 val_total_loss is 0.054066192\n",
      "learning_rate is  0.001\n",
      "The 2900 step train_total_loss is 0.33002466 val_total_loss is 0.16700266\n",
      "learning_rate is  0.001\n",
      "The 2950 step train_total_loss is 1.23582 val_total_loss is 0.4498115\n",
      "learning_rate is  0.001\n",
      "The 3000 step train_total_loss is 0.2997662 val_total_loss is 0.12185902\n",
      "learning_rate is  0.001\n",
      "The 3050 step train_total_loss is 0.6252688 val_total_loss is 0.24034248\n",
      "learning_rate is  0.001\n",
      "The 3100 step train_total_loss is 1.0846354 val_total_loss is 0.9206764\n",
      "learning_rate is  0.001\n",
      "The 3150 step train_total_loss is 0.5411282 val_total_loss is 0.094554216\n",
      "learning_rate is  0.001\n",
      "The 3200 step train_total_loss is 0.10943177 val_total_loss is 0.2610906\n",
      "learning_rate is  0.001\n",
      "The 3250 step train_total_loss is 0.3511835 val_total_loss is 0.6632664\n",
      "learning_rate is  0.001\n",
      "The 3300 step train_total_loss is 0.27945322 val_total_loss is 1.692341\n",
      "learning_rate is  0.001\n",
      "The 3350 step train_total_loss is 0.7697078 val_total_loss is 0.123774044\n",
      "learning_rate is  0.001\n",
      "The 3400 step train_total_loss is 0.84995604 val_total_loss is 0.16626504\n",
      "learning_rate is  0.001\n",
      "The 3450 step train_total_loss is 0.6853421 val_total_loss is 0.23564766\n",
      "learning_rate is  0.001\n",
      "The 3500 step train_total_loss is 0.20723654 val_total_loss is 0.2490126\n",
      "learning_rate is  0.001\n",
      "The 3550 step train_total_loss is 0.348721 val_total_loss is 0.94208175\n",
      "learning_rate is  0.001\n",
      "The 3600 step train_total_loss is 0.7000495 val_total_loss is 1.3611608\n",
      "learning_rate is  0.001\n",
      "The 3650 step train_total_loss is 0.3065457 val_total_loss is 0.36209002\n",
      "learning_rate is  0.001\n",
      "The 3700 step train_total_loss is 0.9457991 val_total_loss is 0.5533474\n",
      "learning_rate is  0.001\n",
      "The 3750 step train_total_loss is 0.093997344 val_total_loss is 0.021324806\n",
      "learning_rate is  0.001\n",
      "The 3800 step train_total_loss is 0.590563 val_total_loss is 0.86632407\n",
      "learning_rate is  0.001\n",
      "The 3850 step train_total_loss is 0.17472872 val_total_loss is 1.0708728\n",
      "learning_rate is  0.001\n",
      "The 3900 step train_total_loss is 0.24287634 val_total_loss is 1.2869086\n",
      "learning_rate is  0.001\n",
      "The 3950 step train_total_loss is 0.41897488 val_total_loss is 0.50876784\n",
      "learning_rate is  0.001\n",
      "The 4000 step train_total_loss is 0.77218676 val_total_loss is 0.31587547\n",
      "learning_rate is  0.001\n",
      "The 4050 step train_total_loss is 0.24392754 val_total_loss is 0.6254512\n",
      "learning_rate is  0.001\n",
      "The 4100 step train_total_loss is 0.2837085 val_total_loss is 0.252351\n",
      "learning_rate is  0.001\n",
      "The 4150 step train_total_loss is 0.14232723 val_total_loss is 0.25504118\n",
      "learning_rate is  0.001\n",
      "The 4200 step train_total_loss is 0.10819234 val_total_loss is 0.29338795\n",
      "learning_rate is  0.001\n",
      "The 4250 step train_total_loss is 1.4967016 val_total_loss is 0.5836458\n",
      "learning_rate is  0.001\n",
      "The 4300 step train_total_loss is 0.7173114 val_total_loss is 0.3352921\n",
      "learning_rate is  0.001\n",
      "The 4350 step train_total_loss is 0.650648 val_total_loss is 1.8716003\n",
      "learning_rate is  0.001\n",
      "The 4400 step train_total_loss is 1.3686178 val_total_loss is 0.68520796\n",
      "learning_rate is  0.001\n",
      "The 4450 step train_total_loss is 0.07548195 val_total_loss is 0.4663486\n",
      "learning_rate is  0.001\n",
      "The 4500 step train_total_loss is 0.15658006 val_total_loss is 0.11417079\n",
      "learning_rate is  0.001\n",
      "The 4550 step train_total_loss is 0.12404058 val_total_loss is 0.13259205\n",
      "learning_rate is  0.001\n",
      "The 4600 step train_total_loss is 0.71831214 val_total_loss is 0.03735375\n",
      "learning_rate is  0.001\n",
      "The 4650 step train_total_loss is 0.1806011 val_total_loss is 0.067259476\n",
      "learning_rate is  0.001\n",
      "The 4700 step train_total_loss is 0.23085071 val_total_loss is 0.759772\n",
      "learning_rate is  0.001\n",
      "The 4750 step train_total_loss is 0.41942665 val_total_loss is 0.46359468\n",
      "learning_rate is  0.001\n",
      "The 4800 step train_total_loss is 0.42011642 val_total_loss is 0.35708392\n",
      "learning_rate is  0.001\n",
      "The 4850 step train_total_loss is 0.4259448 val_total_loss is 0.78019536\n",
      "learning_rate is  0.001\n",
      "The 4900 step train_total_loss is 1.2166146 val_total_loss is 0.2245286\n",
      "learning_rate is  0.001\n",
      "The 4950 step train_total_loss is 0.30057043 val_total_loss is 0.1961171\n",
      "learning_rate is  0.001\n",
      "The 5000 step train_total_loss is 0.06704363 val_total_loss is 0.32195356\n",
      "learning_rate is  1e-04\n",
      "The latency is   17382.477112秒\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Thu Oct 25 09:54:12 2018\n",
    "\n",
    "@author: LongJun\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import config as cfg\n",
    "import os\n",
    "import pascal_voc as pascl\n",
    "import tensorflow.contrib.slim as slim\n",
    "import anchor_generate\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "from anchor_label import anchor_labels_process, labels_generate, anchor_labels_process\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import network\n",
    "import datetime\n",
    "from losslayer import RPN_loss\n",
    "from predict_loss import Predict_loss\n",
    "# Solver Class, used for training\n",
    "# net: the name of backbone net, only support VGG16, more backbone net will be supported in the feature\n",
    "# data/val_data： trian_data/vla_data is a list that consist of dict of ground_truth label\n",
    "# rpn_loss class : used for calculating rpn_loss\n",
    "# predict_loss: used for calculating predict_loss\n",
    "class Solver(object):   \n",
    "    def __init__(self, net ,data, val_data, rpn_loss, predict_loss): \n",
    "        self.net = net\n",
    "        self.data = data\n",
    "        self.val_data = val_data\n",
    "        self.max_iter = cfg.MAX_ITER\n",
    "        self.lr = cfg.LEARNING_RATE\n",
    "        self.rpn_loss = rpn_loss\n",
    "        self.predict_loss = predict_loss\n",
    "        self.lr_change_ITER = cfg.lr_change_ITER\n",
    "        self.summary_iter = cfg.SUMMARY_ITER\n",
    "        self.save_iter = cfg.SAVE_ITER\n",
    "        self.overlaps_max = cfg.overlaps_max\n",
    "        self.overlaps_min = cfg.overlaps_min\n",
    "        self._variables_to_fix = {}\n",
    "        self.Summary_output = os.path.join(cfg.Summary_output, datetime.datetime.now().strftime('%Y_%m_%d_%H_%M'))\n",
    "        if not os._exists(self.Summary_output):\n",
    "            os.mkdir(self.Summary_output)\n",
    "        self.train_summary_dir = os.path.join(self.Summary_output, 'train')\n",
    "        self.val_summary_dir = os.path.join(self.Summary_output, 'val')\n",
    "        self.model_output_dir = os.path.join(cfg.OUTPUT_DIR) \n",
    "        if not os.path.exists(self.model_output_dir):\n",
    "            os.mkdir(self.model_output_dir)\n",
    "        if not os.path.exists(self.train_summary_dir):\n",
    "            os.mkdir(self.train_summary_dir)\n",
    "        if not os.path.exists(self.val_summary_dir):\n",
    "            os.mkdir(self.val_summary_dir)\n",
    "        self.ckpt_filename = os.path.join(self.model_output_dir, 'output.model')\n",
    "        \n",
    " # training process       \n",
    "    def train_model(self):\n",
    "        lr = tf.Variable(self.lr[0],trainable=False)\n",
    "        self.optimizer = tf.train.MomentumOptimizer(lr, cfg.momentum)\n",
    "        #self.optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "        self.loss = self.rpn_loss.add_loss() + self.predict_loss.add_loss()     \n",
    "        train_op = self.optimizer.minimize(self.loss)\n",
    "        variables = tf.global_variables()\n",
    "        reader = pywrap_tensorflow.NewCheckpointReader(self.net.weight_file_path)\n",
    "        var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "        variables_to_restore = self.get_var_list(variables, var_to_shape_map)\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver(var_list=variables_to_restore)\n",
    "        merged = tf.summary.merge_all()\n",
    "        with tf.Session() as sess:\n",
    "            train_writer = tf.summary.FileWriter(self.train_summary_dir, sess.graph)\n",
    "            val_writer = tf.summary.FileWriter(self.val_summary_dir)\n",
    "            sess.run(init)\n",
    "            saver.restore(sess, self.net.weight_file_path)\n",
    "            self.fix_variables(sess, self.net.weight_file_path)\n",
    "            saver = tf.train.Saver(variables,max_to_keep = 10)\n",
    "            for step in range(5000+1):\n",
    "                if step == self.lr_change_ITER:\n",
    "                    lr = tf.assign(lr, self.lr[1])\n",
    "                train_data = self.data.get()\n",
    "                image_height = np.array(train_data['image'].shape[1])\n",
    "                image_width = np.array(train_data['image'].shape[2])\n",
    "                feed_dict = {self.net.image: train_data['image'], self.net.image_width: image_width,\\\n",
    "                             self.net.image_height: image_height, self.net.gt_boxes: train_data['box'],\\\n",
    "                             self.net.gt_cls: train_data['cls']}\n",
    "                if step % self.summary_iter == 0:\n",
    "                    total_loss, summary, learning_rate= sess.run([self.loss, merged, lr], feed_dict=feed_dict)\n",
    "                    train_writer.add_summary(summary, step)\n",
    "                    val_data = self.val_data.get()\n",
    "                    val_image_height = np.array(val_data['image'].shape[1])\n",
    "                    val_image_width = np.array(val_data['image'].shape[2])\n",
    "                    val_feed_dict = {self.net.image: val_data['image'], self.net.image_width: val_image_width,\\\n",
    "                                     self.net.image_height: val_image_height, self.net.gt_boxes: val_data['box'],\\\n",
    "                                     self.net.gt_cls: val_data['cls']}\n",
    "                    val_loss, val_summary = sess.run([self.loss, merged], feed_dict=val_feed_dict)\n",
    "                    val_writer.add_summary(val_summary, step)\n",
    "                    print ('The', step, 'step train_total_loss is', total_loss, 'val_total_loss is', val_loss)\n",
    "                    print ('learning_rate is ', learning_rate)\n",
    "                if step % self.save_iter == 0:\n",
    "                    saver.save(sess, self.ckpt_filename, global_step = step)\n",
    "                sess.run(train_op, feed_dict=feed_dict)\n",
    "                    \n",
    "               \n",
    "                \n",
    "                \n",
    "#get the variables to restore               \n",
    "    def get_var_list(self, global_variables, ckpt_variables):\n",
    "        variables_to_restore = []\n",
    "        for key in global_variables:\n",
    "            print (key.name)\n",
    "            if key.name == ('vgg_16/fc6/weights:0') or key.name == ('vgg_16/fc7/weights:0'):\n",
    "                self._variables_to_fix[key.name] = key\n",
    "                continue\n",
    "            \n",
    "            if key.name.split(':')[0] in ckpt_variables:\n",
    "                variables_to_restore.append(key) \n",
    "        return variables_to_restore\n",
    "    \n",
    "#because fc6 and fc7 layers of pretrained vgg16 model is convolution format, so we need convert them to fully-connected layers\n",
    "    def fix_variables(self, sess, pretrained_model):\n",
    "        print('Fix VGG16 layers..')\n",
    "        with tf.variable_scope('Fix_VGG16') as scope:\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                fc6_conv = tf.get_variable(\"fc6_conv\", [7, 7, 512, 4096], trainable=False)\n",
    "                fc7_conv = tf.get_variable(\"fc7_conv\", [1, 1, 4096, 4096], trainable=False)\n",
    "                restorer_fc = tf.train.Saver({'vgg_16' + \"/fc6/weights\": fc6_conv, \n",
    "                                              'vgg_16' + \"/fc7/weights\": fc7_conv})\n",
    "                restorer_fc.restore(sess, pretrained_model)\n",
    "        \n",
    "                sess.run(tf.assign(self._variables_to_fix['vgg_16' + '/fc6/weights:0'], tf.reshape(fc6_conv, \n",
    "                                    self._variables_to_fix['vgg_16' + '/fc6/weights:0'].get_shape())))\n",
    "                sess.run(tf.assign(self._variables_to_fix['vgg_16' + '/fc7/weights:0'], tf.reshape(fc7_conv, \n",
    "                                    self._variables_to_fix['vgg_16' + '/fc7/weights:0'].get_shape())))\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = cfg.GPU_ID\n",
    "    net = network.Net()\n",
    "    rpn_loss_obj = RPN_loss(net.rois_output['rois_bbx'], net.all_anchors, net.gt_boxes, \\\n",
    "                        net.rois_output['rois_cls'], net.labels, net.anchor_obj)\n",
    "    predict_loss = Predict_loss(net._predictions[\"cls_score\"], net._proposal_targets['labels'],\\\n",
    "                                net._predictions['bbox_pred'], net._proposal_targets['bbox_targets'],\\\n",
    "                                net._proposal_targets['bbox_inside_weights'], net._proposal_targets['bbox_outside_weights'])\n",
    "    \n",
    "    train_data = pascl.pascal_voc(cfg.train_imdb_name, 'train', fliped=True)\n",
    "    val_data = pascl.pascal_voc(cfg.test_imdb_name, 'test', fliped=False)\n",
    "    solver = Solver(net, train_data, val_data, rpn_loss_obj, predict_loss)\n",
    "    print ('start training')\n",
    "    start = datetime.datetime.now()\n",
    "    solver.train_model()\n",
    "    end = datetime.datetime.now()\n",
    "    latency = (end - start).total_seconds()  \n",
    "    print(\"The latency is   \"+str(latency)+\"秒\")  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
