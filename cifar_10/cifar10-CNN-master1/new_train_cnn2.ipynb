{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/code/cifar_10/cifar10-CNN-master1/cifar10_input.py:65: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/code/cifar_10/cifar10-CNN-master1/cifar10_input.py:68: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/training/input.py:277: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "这是filename_queue <tensorflow.python.ops.data_flow_ops.FIFOQueue object at 0x7fc823232e48>\n",
      "WARNING:tensorflow:From /root/code/cifar_10/cifar10-CNN-master1/cifar10_input.py:40: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\n",
      "这是读到的数据read_input  <cifar10_input.read_cifar10.<locals>.CIFAR10Record object at 0x7fc940ae8860>\n",
      "WARNING:tensorflow:From /root/code/cifar_10/cifar10-CNN-master1/cifar10_input.py:81: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /root/code/cifar_10/cifar10-CNN-master1/cifar10_input.py:98: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n",
      "WARNING:tensorflow:From /root/code/cifar_10/cifar10-CNN-master1/cifar10_input.py:99: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/code/cifar_10/cifar10-CNN-master1/cifar10_input.py:129: The name tf.image.resize_image_with_crop_or_pad is deprecated. Please use tf.image.resize_with_crop_or_pad instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/code/cifar_10/cifar10-CNN-master1/cifar10_input.py:139: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From <ipython-input-1-78b05c9879ce>:122: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "Epoch :  100 loss :  1471614.75000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary +: 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-78b05c9879ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimization finished ! 总耗时\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;31m#display loss processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary +: 'str'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import cifar10, cifar10_input\n",
    " \n",
    "#define patameters\n",
    "max_steps = 100\n",
    "\n",
    "batch_size = 512\n",
    "n_fc_1 = 128\n",
    "n_fc_2 = 64\n",
    "display_step = 200\n",
    "data_dir = 'cifar10_data/cifar-10-batches-bin/'\n",
    " \n",
    "#define L2 regularization for weight loss\n",
    "def l2_weight_loss(shape, stddev, w_1):\n",
    "    '''\n",
    "description: to adopt L2 regularization for weight to prevent over-fitting\n",
    "Args:\tshape:\n",
    "stddev: standard deviation\n",
    "w_1: weight coeffient\n",
    "Returns:\n",
    "weight: the regularized weight coeffient\n",
    "'''\n",
    "    weight = tf.Variable(tf.truncated_normal(shape, stddev))\n",
    "    if w_1 is not None:\n",
    "        weight_loss = tf.multiply(tf.nn.l2_loss(weight), w_1, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_loss)\n",
    "    return weight\n",
    " \n",
    "#define weight initializer\n",
    "def weight_init(shape, stddev):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev))\n",
    " \n",
    "#define biases initializer\n",
    "def biases_init(shape):\n",
    "\n",
    "    return tf.Variable(tf.random_normal(shape))\n",
    " \n",
    "#define conv layer\n",
    "def conv2d(x_image, weight):\n",
    "\n",
    "    return tf.nn.conv2d(x_image, weight, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    " \n",
    "#define max pooling layer\n",
    "def max_pool(x_image):\n",
    "\n",
    "    return tf.nn.max_pool(x_image, ksize = [1, 3, 3, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    " \n",
    "def lrn_norm(x_image):\n",
    "\n",
    "    return tf.nn.lrn(x_image, 4, bias = 1.0, alpha = 0.001 / 9.0, beta = 0.75)\n",
    " \n",
    "#image enhancement processing for trainning data sets\n",
    "train_images, train_labels = cifar10_input.distorted_inputs(batch_size = batch_size, data_dir = data_dir)\n",
    " \n",
    "#define input placeholder\n",
    "x_images = tf.placeholder(tf.float32, [batch_size, 24, 24, 3],name='x_images')\n",
    "x_labels = tf.placeholder(tf.float32, [batch_size],name='x_labels')\n",
    "keep_prob = tf.placeholder(\"float\",name='keep_prob') \n",
    "#define layer 1\n",
    "w_1 = weight_init([5, 5, 3, 32], 0.05)\n",
    "b_1 = biases_init([32])\n",
    "conv_1 = tf.nn.relu(conv2d(x_images, w_1) + b_1)\n",
    "pool_1 = max_pool(conv_1)\n",
    "lrn_1 = lrn_norm(pool_1)\n",
    " \n",
    "#define layer 2\n",
    "w_2 = weight_init([5, 5, 32, 32], 0.05)\n",
    "b_2 = biases_init([32])\n",
    "conv_2 = tf.nn.relu(conv2d(lrn_1, w_2) + b_2)\n",
    "pool_2 = max_pool(conv_2)\n",
    " \n",
    "#define flatten layer\n",
    "re_shape = tf.reshape(pool_2, [batch_size, -1])\n",
    "n_input = re_shape.get_shape()[1].value\n",
    " \n",
    "#define full connection 1\n",
    "w_3 = l2_weight_loss([n_input, n_fc_1], 0.05, w_1 = 0.001)\n",
    "b_3 = biases_init([n_fc_1])\n",
    "fc_1 = tf.nn.relu(tf.matmul(re_shape, w_3) + b_3)\n",
    " \n",
    "#define full connection 2\n",
    "w_4 = l2_weight_loss([n_fc_1, n_fc_2], 0.05, w_1 = 0.003)\n",
    "b_4 = biases_init([n_fc_2])\n",
    "fc_2 = tf.nn.relu(tf.matmul(fc_1, w_4) + b_4)\n",
    " \n",
    "#define output layer\n",
    "w_5 = weight_init([n_fc_2, 10], 1.0 / 96.0)\n",
    "b_5 = biases_init([10])\n",
    "logits = tf.add(tf.matmul(fc_2, w_5), b_5)\n",
    "y_pred = tf.nn.softmax(logits,name='y_pred')\n",
    " \n",
    "#define loss function\n",
    "x_labels = tf.cast(x_labels, tf.int32)\n",
    "cross_enerty = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = x_labels, name = 'cross_enerty_pre_example')\n",
    "losses = tf.reduce_mean(cross_enerty, name = 'cross_enerty')\n",
    "tf.add_to_collection('losses', losses)\n",
    "loss = tf.add_n(tf.get_collection('losses'), name = 'total_loss')\n",
    " \n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.0001).minimize(loss)\n",
    "saver = tf.train.Saver() #定义saver\n",
    "#display test result\n",
    "test_images, test_labels = cifar10_input.inputs(batch_size = batch_size, data_dir = data_dir)\n",
    " \n",
    "#claculate trainning accuracy\n",
    "def accuracy(test_labels, y_pred):\n",
    "\n",
    "    test_labels = tf.to_int64(test_labels)\n",
    "    correction_pred = tf.equal(test_labels, tf.argmax(y_pred, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(correction_pred, tf.float32))\n",
    "    return acc\n",
    " \n",
    "#create session\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    " \n",
    "#create thread to accelaeate processing efficience\n",
    "tf.train.start_queue_runners(sess = sess)\n",
    " \n",
    "#start trainning and display\n",
    "cross_loss = []\n",
    "start_time = time.time()\n",
    "for i in range(max_steps):\n",
    "\n",
    "    batch_xs, batch_ys = sess.run([train_images, train_labels])\n",
    "#     print(batch_xs,batch_ys)\n",
    "    _, c = sess.run([optimizer, loss], feed_dict = {x_images:batch_xs, x_labels:batch_ys})\n",
    "    cross_loss.append(c)\n",
    "    every_epoch_time = time.time() - start_time\n",
    "    if i % display_step == 0:\n",
    "        examples_per_sec = batch_size / every_epoch_time\n",
    "        every_batch_time = float(every_epoch_time)\n",
    "\n",
    "        print('Epoch : ', '%d'%(i+100), 'loss : ', '{:.5f}'.format(c))\n",
    "        \n",
    "cost_time = time.time() - start_time\n",
    "\n",
    "saver.save(sess, '/tmp/tf_cifar_cnn2/model.ckpt') #模型储存位置\n",
    "\n",
    "\n",
    "print(\"optimization finished ! 总耗时\",+str(cost_time))\n",
    " \n",
    "#display loss processing\n",
    "# fig, ax = plt.subplots(13, 6)\n",
    "# ax.plot(cross_loss)\n",
    "# plt.grid()\n",
    "# plt.title(\"trian loss\")\n",
    "# plt.show() \n",
    " \n",
    "#claculate test accuracy\n",
    "for i in range(10):\n",
    "    test_acc = []\n",
    "    batch_xs, batch_ys = sess.run([test_images, test_labels])\n",
    "    batch_y_pred = sess.run(y_pred, feed_dict = {x_images:batch_xs})\n",
    "#     print('这是batch_y_pred',batch_y_pred)\n",
    "#     print('这是batch_ys',batch_ys)\n",
    "\n",
    "\n",
    "    test_accuracy = accuracy(batch_ys, batch_y_pred)\n",
    "    \n",
    "    acc = sess.run(test_accuracy, feed_dict = {x_images:batch_xs})\n",
    "#     train_accuracy = acc.eval(feed_dict={x_images:batch_xs, y_pred: batch_ys,keep_prob: 1.0},session=sess)\n",
    "    test_acc.append(acc)\n",
    "    print(\"test accuracy : \", acc)\n",
    "#     print(\"test accuracy : \", acc,'train_accuracy',train_accuracy)\n",
    "\n",
    "print(\"mean accuracy : \", np.mean(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #claculate trainning accuracy\n",
    "# def accuracy(test_labels, y_pred):\n",
    "\n",
    "#     test_labels = tf.to_int64(test_labels)\n",
    "#     correction_pred = tf.equal(test_labels, tf.argmax(y_pred, 1))\n",
    "#     acc = tf.reduce_mean(tf.cast(correction_pred, tf.float32))\n",
    "#     return acc\n",
    " \n",
    "# #claculate test accuracy\n",
    "# for i in range(10):\n",
    "#     test_acc = []\n",
    "#     batch_xs, batch_ys = sess.run([test_images, test_labels])\n",
    "#     batch_y_pred = sess.run(y_pred, feed_dict = {x_images:batch_xs})\n",
    "#     acc = accuracy(batch_ys, batch_y_pred)\n",
    "#     test_acc.append(acc)\n",
    "#     print(\"test accuracy : \", acc)\n",
    "# print(\"mean accuracy : \", np.mean(test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是batch_y_pred [[0. 0. 0. ... 0. 1. 0.]\n",
    " [0. 0. 0. ... 0. 1. 0.]\n",
    " [0. 0. 0. ... 0. 1. 0.]\n",
    " ...\n",
    " [0. 0. 0. ... 0. 0. 0.]\n",
    " [0. 0. 0. ... 0. 0. 0.]\n",
    " [0. 0. 0. ... 0. 0. 0.]]\n",
    "这是batch_ys [3 8 8 0 6 6 1 6 3 1 0 9 5 7 9 8 5 7 8 6 7 0 4 9 5 2 4 0 9 6 6 5 4 5 9 2 4\n",
    " 1 9 5 4 6 5 6 0 9 3 9 7 6 9 8 0 3 8 8 7 7 4 6 7 3 6 3 6 2 1 2 3 7 2 6 8 8\n",
    " 0 2 9 3 3 8 8 1 1 7 2 5 2 7 8 9 0 3 8 6 4 6 6 0 0 7 4 5 6 3 1 1 3 6 8 7 4\n",
    " 0 6 2 1 3 0 4 2 7 8 3 1 2 8 0 8 3 5 2 4 1 8 9 1 2 9 7 2 9 6 5 6 3 8 7 6 2\n",
    " 5 2 8 9 6 0 0 5 2 9 5 4 2 1 6 6 8 4 8 4 5 0 9 9 9 8 9 9 3 7 5 0 0 5 2 2 3\n",
    " 8 6 3 4 0 5 8 0 1 7 2 8 8 7 8 5 1 8 7 1 3 0 5 7 9 7 4 5 9 8 0 7 9 8 2 7 6\n",
    " 9 4 3 9 6 4 7 6 5 1 5 8 8 0 4 0 5 5 1 1 8 9 0 3 1 9 2 2 5 3 9 9 4 0 3 0 0\n",
    " 9 8 1 5 7 0 8 2 4 7 0 2 3 6 3 8 5 0 3 4 3 9 0 6 1 0 9 1 0 7 9 1 2 6 9 3 4\n",
    " 6 0 0 6 6 6 3 2 6 1 8 2 1 6 8 6 8 0 4 0 7 7 5 5 3 5 2 3 4 1 7 5 4 6 1 9 3\n",
    " 6 6 9 3 8 0 7 2 6 2 5 8 5 4 6 8 9 9 1 0 2 2 7 3 2 8 0 9 5 8 1 9 4 1 3 8 1\n",
    " 4 7 9 4 2 7 0 7 0 6 6 9 0 9 2 8 7 2 2 5 1 2 6 2 9 6 2 3 0 3 9 8 7 8 8 4 0\n",
    " 1 8 2 7 9 3 6 1 9 0 7 3 7 4 5 0 0 2 9 3 4 0 6 2 5 3 7 3 7 2 5 3 1 1 4 9 9\n",
    " 5 7 5 0 2 2 2 9 7 3 9 4 3 5 4 6 5 6 1 4 3 4 4 3 7 8 3 7 8 0 5 7 6 0 5 4 8\n",
    " 6 8 5 5 9 9 9 5 0 1 0 8 1 1 8 0 2 2 0 4 6 5 4 9 4 7 9 9 4 5 6]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
